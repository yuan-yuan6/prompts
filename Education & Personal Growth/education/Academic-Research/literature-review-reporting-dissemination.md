---
category: education
related_templates:
- education/Academic-Research/literature-review-analysis-implications.md
- education/Academic-Research/literature-review-protocol-search.md
- education/Academic-Research/literature-reviews-overview.md
tags:
- review-reporting
- prisma-guidelines
- knowledge-translation
- research-dissemination
- capability-assessment
title: Literature Review Reporting & Dissemination Readiness Assessment
use_cases:
- Evaluating readiness to write and publish a literature review with transparent reporting
- Translating review findings into stakeholder-specific dissemination products
- Planning knowledge translation with realistic channels, timelines, and ownership
- Designing impact measurement for dissemination and uptake
industries:
- education
- government
- healthcare
- technology
type: framework
difficulty: intermediate
slug: literature-review-reporting-dissemination
---

# Literature Review Reporting & Dissemination Readiness Assessment

## Purpose
Assess whether a completed literature review is ready to be reported and disseminated in a way that is compliant with reporting standards, credible to peer review, and useful to stakeholders. This framework helps ensure that methods are transparent, findings are communicated with appropriate uncertainty, and dissemination products match the needs of real audiences.

## üöÄ Quick Prompt

> Assess **reporting & dissemination readiness** for **{REVIEW_CONTEXT}** with intended outputs for **{TARGET_AUDIENCES}** delivered as **{DELIVERABLE_SET}**. Evaluate: (1) reporting standard alignment (PRISMA/ENTREQ/RAMESES), (2) results clarity and uncertainty communication, (3) transparency and reproducibility artifacts, (4) stakeholder tailoring and message discipline, (5) dissemination plan and channel fit, and (6) impact measurement and maintenance (updates, living review considerations). Provide a scorecard (1‚Äì5), top risks, a minimum viable manuscript outline, and a practical dissemination plan with owners and timelines.

---

## Template

Conduct a reporting and dissemination readiness assessment for {REVIEW_CONTEXT} for {TARGET_AUDIENCES} delivered as {DELIVERABLE_SET}.

Score each dimension from 1.0 to 5.0 and justify scores with evidence (draft manuscript outline, checklist mapping, PRISMA flow data, tables/figures drafts, plain-language summary draft, stakeholder review notes, and a channel/timeline plan).

### 1) Reporting Standard Alignment Readiness
Assess whether reporting will be compliant and complete by evaluating whether the correct reporting guideline is selected for the review type (PRISMA for systematic reviews, ENTREQ for qualitative syntheses, RAMESES for realist syntheses, or equivalents), whether required items can be satisfied with existing tracking data (search details, selection reasons, risk of bias, synthesis methods), whether the protocol and deviations can be reported transparently, whether conflicts of interest and funding statements are prepared, and whether authorship and contribution roles are defined.

A strong score implies you can complete the relevant reporting checklist without retroactively inventing missing methodological detail.

### 2) Results Clarity & Uncertainty Communication Readiness
Evaluate whether the results can be communicated responsibly by assessing whether key outcomes/themes are clearly prioritized, whether effect estimates (or thematic findings) are presented with uncertainty and limitations, whether heterogeneity and contradictory findings are explained rather than hidden, whether ‚Äúno evidence‚Äù is distinguished from ‚Äúevidence of no effect,‚Äù whether certainty ratings (e.g., GRADE/CERQual) are summarized in a way stakeholders can understand, and whether conclusions are proportional to evidence strength.

A strong score implies readers can understand what is known, what is uncertain, and what decisions the evidence can and cannot support.

### 3) Transparency, Reproducibility & Materials Readiness
Assess whether the work can be audited and reproduced by evaluating whether search strategies are documented per database, whether excluded studies and reasons are traceable, whether extraction tables and risk-of-bias assessments are available (main or supplementary), whether analysis code and data transformations are documented (or at least the steps are reproducible), whether supplementary materials are planned and organized, and whether data sharing decisions are explicit (what can be shared and under what constraints).

A strong score implies the paper can withstand methodological scrutiny and be updated later without rebuilding from scratch.

### 4) Stakeholder Tailoring & Message Discipline Readiness
Evaluate whether the story matches the audience by assessing whether the manuscript narrative is coherent (question ‚Üí methods ‚Üí results ‚Üí implications), whether stakeholder-specific outputs are consistent with the evidence (no over-interpretation in policy briefs or press releases), whether language is accessible where required (plain language summaries), whether equity and implementation considerations are addressed for practice/policy audiences, and whether messaging is disciplined around a small number of defensible takeaways.

A strong score implies each audience receives an honest, usable interpretation without distortion.

### 5) Dissemination Plan & Channel Fit Readiness
Assess whether dissemination is realistic and likely to reach the intended audiences by evaluating whether channels match stakeholder workflows (journals, conferences, professional societies, policy briefings, webinars), whether products are defined with scope and owners (manuscript, one-pager, brief, deck, infographic), whether timelines align to decision cycles, whether stakeholder engagement is planned (who reviews what and when), and whether resourcing is adequate for production-quality assets.

A strong score implies dissemination is not an afterthought; it is planned like a deliverable with accountability.

### 6) Impact Measurement, Uptake & Maintenance Readiness
Evaluate whether you can measure and sustain impact by assessing whether impact metrics are defined per channel (downloads, citations, policy mentions, training adoption, practice uptake), whether feedback loops exist to correct misinterpretations, whether plans exist for updates (living review triggers, surveillance cadence), whether implementation support is considered when relevant (toolkits, checklists, decision aids), and whether ownership is assigned for ongoing maintenance.

A strong score implies you can demonstrate reach and influence while keeping the evidence communication accurate over time.

---

## Required Output Format

Provide:

1) Executive summary with overall readiness (X.X/5.0), maturity stage, and top 3 blockers.

2) Dimension scorecard with evidence-based rationale.

3) Minimum viable manuscript outline aligned to the chosen reporting standard.

4) Figure/table plan listing required artifacts (PRISMA flow, study characteristics, risk-of-bias, key results, certainty summary).

5) Dissemination plan with deliverables, channels, owners, and a timeline.

6) Stakeholder tailoring notes: what changes by audience and what must not change.

7) Impact measurement plan describing metrics, collection method, and reporting cadence.

---

## Maturity Scale

- 1.0‚Äì1.9: Unprepared (missing tracking data; unclear messages; weak transparency)
- 2.0‚Äì2.9: Drafting (core content exists; checklists and outputs incomplete)
- 3.0‚Äì3.9: Submission-ready (standards met; clear uncertainty communication; defined products)
- 4.0‚Äì4.9: Stakeholder-ready (tailored outputs; dissemination plan and metrics operational)
- 5.0: Field-leading (high-impact dissemination; durable artifacts; maintainable evidence updates)

---

## Variables

| Variable | Description | Examples |
|----------|-------------|----------|
| `{REVIEW_CONTEXT}` | Topic and scope of the review | "Telehealth for chronic disease management", "School mental health programs" |
| `{TARGET_AUDIENCES}` | Who will consume the outputs | "Academics + guideline panel + policymakers", "Practitioners + public" |
| `{DELIVERABLE_SET}` | What will be produced | "Journal manuscript + policy brief + plain language summary", "Deck + infographic + webinar" |

---

## Usage Example (Single Example)

**Scenario:** A team completed {REVIEW_CONTEXT} and wants to communicate results to {TARGET_AUDIENCES} using {DELIVERABLE_SET}.

**Scores (1‚Äì5):** Reporting Standard Alignment 3.0, Results/Uncertainty 2.7, Transparency/Reproducibility 2.5, Stakeholder Tailoring 2.8, Dissemination Plan 2.4, Impact & Maintenance 2.2. Overall readiness: 2.6/5.0 (Drafting).

**Key findings:** The review can be written, but key tracking artifacts are incomplete (database-specific search strings, exclusion reasons, and supplementary extraction tables), which will slow submission and increase risk during peer review. The dissemination plan is vague and not tied to stakeholder decision cycles. The fastest readiness gains are to map the manuscript to the correct reporting checklist, build the missing evidence trail, and define a realistic deliverables timeline with owners and a minimal impact tracking plan.

---

## Related Resources

- [Literature Review Analysis & Implications](education/Academic-Research/literature-review-analysis-implications.md)
- [Literature Review Protocol & Search Strategy](education/Academic-Research/literature-review-protocol-search.md)
- [Literature Reviews Overview](education/Academic-Research/literature-reviews-overview.md)
