---
category: product-management/Product-Analytics
last_updated: 2025-11-12
title: A/B Testing & Experimentation
tags:
- product-management
- ab-testing
- experimentation
- analytics
- optimization
use_cases:
- Designing and running A/B tests to validate product hypotheses
- Optimizing conversion funnels and user flows
- Making data-driven product decisions through experimentation
- Building experimentation culture and frameworks
related_templates:
- product-management/product-metrics-kpis.md
- product-management/Product-Analytics/user-behavior-analysis.md
- product-management/Product-Analytics/product-analytics-framework.md
- product-management/Product-Development/product-requirements-document.md
industries:
- technology
- finance
- healthcare
- retail
- manufacturing
---

# A/B Testing & Experimentation Template

## Purpose
Design rigorous A/B tests and experiments to validate product hypotheses, optimize user experiences, and make data-driven decisions with statistical confidence rather than opinions or assumptions.

## Quick Start

**Need to run an A/B test quickly?** Use this streamlined approach:

### Minimal Example
```
Hypothesis: Changing CTA from "Sign Up" to "Start Free Trial" increases conversions
Metric: Signup conversion rate
Baseline: 12% conversion
Sample: 10,000 users (5,000 per variant)
Duration: 2 weeks
Success criteria: >15% increase (>13.8% conversion), 95% confidence
Result: 14.5% conversion, +2.5pp, 98% confidence → SHIP IT
```

### When to Use This
- Testing new features before full rollout
- Optimizing conversion funnels
- Validating design or copy changes
- Deciding between product alternatives
- Building evidence for stakeholder decisions

### Basic 5-Step Workflow
1. **Hypothesis formation** - What you believe and why (1 day)
2. **Experiment design** - Define variants, metrics, sample size (2 days)
3. **Implementation** - Build variants and instrumentation (1 week)
4. **Run experiment** - Collect data to statistical significance (2-4 weeks)
5. **Analysis & decision** - Interpret results and ship or kill (2 days)

---

## Template

```
You are an experienced experimentation scientist and product manager. Design and analyze an A/B test for [PRODUCT_NAME] testing [HYPOTHESIS] with [TARGET_USERS] measuring [PRIMARY_METRIC] to inform [DECISION] with [STATISTICAL_RIGOR].

EXPERIMENT CONTEXT:
Product Information:
- Product name: [PRODUCT_NAME]
- Product area: [FEATURE/PAGE/FLOW]
- Current state: [BASELINE_BEHAVIOR]
- Traffic: [DAILY/MONTHLY_USERS]
- Test environment: [WEB/MOBILE/API]

Experiment Objective:
- Decision to make: [WHAT_YOU'RE_DECIDING]
- Hypothesis: [WHAT_YOU_BELIEVE]
- Expected impact: [PREDICTED_OUTCOME]
- Why now: [RATIONALE_FOR_TESTING]

### 1. HYPOTHESIS DEVELOPMENT

Problem Statement:
Current state: [WHAT'S_HAPPENING_NOW]
- Data/evidence: [SUPPORTING_EVIDENCE]
- User feedback: [QUALITATIVE_INSIGHTS]
- Business impact: [COST_OF_PROBLEM]

Hypothesis:
If we [CHANGE_DESCRIPTION],
Then we will see [EXPECTED_OUTCOME],
Because [UNDERLYING_REASONING].

Example:
"If we change the signup button from 'Sign Up' to 'Start Free Trial',
Then we will see a 15% increase in signup conversion rate,
Because users will better understand there's no upfront commitment required."

Alternative Hypotheses:
- Hypothesis A: [VARIATION_1]
- Hypothesis B: [VARIATION_2]
- Hypothesis C: [VARIATION_3]

Rationale for chosen hypothesis: [WHY_THIS_ONE]

### 2. EXPERIMENT DESIGN

Test Type:
- Type: [A/B/A/B/n/MULTIVARIATE/BANDIT]
- Why this type: [RATIONALE]

Variants:
Control (A):
- Description: [CURRENT_EXPERIENCE]
- Screenshot/mockup: [VISUAL]
- Implementation: [WHAT_TO_BUILD]

Variant B:
- Description: [CHANGED_EXPERIENCE]
- Changes: [WHAT'S_DIFFERENT]
- Screenshot/mockup: [VISUAL]
- Implementation: [WHAT_TO_BUILD]

Variant C (if applicable):
- Description: [ALTERNATIVE_EXPERIENCE]
- Changes: [WHAT'S_DIFFERENT]

Target Audience:
- Who: [USER_SEGMENT]
- Inclusion criteria: [WHO_GETS_INCLUDED]
- Exclusion criteria: [WHO_GETS_EXCLUDED]
- Geographic: [REGIONS]
- Platform: [WEB/MOBILE/BOTH]

Traffic Allocation:
- Control: [PERCENTAGE]
- Variant B: [PERCENTAGE]
- Variant C: [PERCENTAGE] (if applicable)
- Holdout: [PERCENTAGE] (if applicable)

Rationale: [WHY_THIS_SPLIT]

### 3. SUCCESS METRICS

Primary Metric:
Metric: [METRIC_NAME]
- Definition: [EXACT_CALCULATION]
- Current baseline: [VALUE]
- Minimum Detectable Effect (MDE): [PERCENTAGE_OR_ABSOLUTE]
- Significance level: [95%_OR_99%]
- Statistical power: [80%_OR_90%]

Why this metric: [RATIONALE]

Example:
"Signup conversion rate"
- Definition: (Users who completed signup / Users who viewed signup page) × 100
- Baseline: 12%
- MDE: 15% relative increase (absolute: 1.8pp to 13.8%)
- Significance: 95% confidence
- Power: 80%

Secondary Metrics:
Metric 1: [METRIC_NAME]
- Definition: [CALCULATION]
- Baseline: [VALUE]
- Expected direction: [INCREASE/DECREASE/NO_CHANGE]
- Why tracking: [RATIONALE]

Metric 2: [METRIC_NAME]
(Same structure)

Metric 3: [METRIC_NAME]
(Same structure)

Guardrail Metrics:
Metric 1: [METRIC_NAME]
- Definition: [CALCULATION]
- Acceptable range: [MIN_TO_MAX]
- Why critical: [WHAT_YOU'RE_PROTECTING]

Example:
"Page load time"
- Definition: Time from request to fully loaded
- Acceptable: <3 seconds (no degradation from baseline)
- Critical: Performance impacts all conversion

Metric 2: [METRIC_NAME]
(Same structure)

### 4. SAMPLE SIZE CALCULATION

Power Analysis:
- Baseline conversion rate: [%]
- Minimum Detectable Effect: [%]
- Significance level (alpha): [0.05]
- Power (1-beta): [0.80]
- One-tailed or two-tailed: [TEST_TYPE]

Required Sample Size:
- Per variant: [N]
- Total: [N × NUMBER_OF_VARIANTS]

Current Traffic:
- Daily users: [COUNT]
- Days to reach sample: [DURATION]

Timeline:
- Start date: [DATE]
- Minimum duration: [DAYS]
- Expected end date: [DATE]
- Maximum duration: [DAYS] (if inconclusive)

Sample Size Formula:
n = 2 × (Z_alpha + Z_beta)² × p × (1-p) / (MDE)²

Where:
- Z_alpha = 1.96 (for 95% confidence)
- Z_beta = 0.84 (for 80% power)
- p = baseline conversion rate
- MDE = minimum detectable effect

### 5. RANDOMIZATION STRATEGY

Randomization Unit:
- Unit: [USER/SESSION/PAGE_VIEW/ACCOUNT]
- Why this unit: [RATIONALE]
- Consistency: [HOW_TO_ENSURE_SAME_EXPERIENCE]

Randomization Method:
- Method: [HASH/RANDOM_NUMBER/WEIGHTED]
- Implementation: [TECHNICAL_APPROACH]
- Cookie/ID used: [IDENTIFIER]
- Persistence: [DURATION]

Avoiding Bias:
- Time-based bias: [HOW_ADDRESSED]
- Selection bias: [HOW_AVOIDED]
- Novelty effect: [HOW_MANAGED]
- Primacy effect: [HOW_HANDLED]

### 6. IMPLEMENTATION PLAN

Technical Implementation:
Experimentation Platform:
- Platform: [OPTIMIZELY/GOOGLE_OPTIMIZE/CUSTOM]
- Feature flags: [SYSTEM]
- A/B testing framework: [TOOL]

Code Changes:
- Frontend: [CHANGES_NEEDED]
- Backend: [CHANGES_NEEDED]
- Mobile: [CHANGES_NEEDED]

Event Tracking:
Event 1: [EVENT_NAME]
- Trigger: [WHEN_IT_FIRES]
- Properties: [DATA_CAPTURED]
- Used for: [WHICH_METRICS]

Event 2: [EVENT_NAME]
(Same structure)

QA Checklist:
- [ ] Variants render correctly
- [ ] Tracking fires correctly
- [ ] Randomization works as expected
- [ ] Users assigned consistently
- [ ] No performance degradation
- [ ] Mobile responsive
- [ ] Accessible
- [ ] Edge cases handled

### 7. EXPERIMENT EXECUTION

Pre-Flight Checklist:
- [ ] Hypothesis documented
- [ ] Metrics defined and instrumented
- [ ] Sample size calculated
- [ ] Variants built and QA'd
- [ ] Randomization tested
- [ ] Stakeholder alignment
- [ ] Analytics dashboard ready
- [ ] Experiment plan reviewed
- [ ] Go/no-go decision made

Launch:
- Launch date: [DATE]
- Launch time: [TIME]
- Traffic ramp: [IMMEDIATE/GRADUAL]
  - If gradual: [RAMP_SCHEDULE]
- Monitoring plan: [FREQUENCY]

Monitoring During Experiment:
Daily:
- Sample size progress: [% TO TARGET]
- Metric trends: [DIRECTION]
- Technical issues: [ERRORS/BUGS]
- Guardrail metrics: [WITHIN_BOUNDS]

Weekly:
- Interim analysis: [TRENDS_EMERGING]
- Segment analysis: [ANY_DIFFERENCES]
- Feedback collection: [QUALITATIVE_INSIGHTS]

Early Stopping Criteria:
Stop for success:
- Primary metric shows [X]% improvement
- Statistical significance reached
- Guardrails all green
- Business case compelling

Stop for harm:
- Guardrail metric breached
- Primary metric shows [X]% degradation
- Critical bug discovered
- Business reason to stop

### 8. ANALYSIS FRAMEWORK

Statistical Analysis:
Primary Metric Analysis:
- Control: [MEAN], [STANDARD_DEVIATION], [CONFIDENCE_INTERVAL]
- Variant: [MEAN], [STANDARD_DEVIATION], [CONFIDENCE_INTERVAL]
- Difference: [ABSOLUTE_DIFFERENCE], [RELATIVE_DIFFERENCE]
- P-value: [VALUE]
- Statistical significance: [YES/NO]
- Confidence interval: [LOWER_BOUND to UPPER_BOUND]

Statistical Test:
- Test used: [T-TEST/CHI-SQUARE/Z-TEST]
- Assumptions met: [NORMALITY/INDEPENDENCE]
- Why this test: [RATIONALE]

Secondary Metrics:
- [METRIC_1]: Control [VALUE] vs Variant [VALUE], p=[P-VALUE]
- [METRIC_2]: Control [VALUE] vs Variant [VALUE], p=[P-VALUE]
- [METRIC_3]: Control [VALUE] vs Variant [VALUE], p=[P-VALUE]

Guardrail Metrics:
- [METRIC_1]: [STATUS] - [VALUE]
- [METRIC_2]: [STATUS] - [VALUE]

Segmentation Analysis:
Segment 1: [SEGMENT_NAME]
- Primary metric: [RESULT]
- Statistical significance: [YES/NO]
- Sample size: [N]

Segment 2: [SEGMENT_NAME]
(Same structure)

Segment 3: [SEGMENT_NAME]
(Same structure)

Insights:
- Which segments benefited most: [ANALYSIS]
- Which segments showed no effect: [ANALYSIS]
- Surprising findings: [OBSERVATIONS]

### 9. RESULTS INTERPRETATION

Results Summary:
Outcome: [SUCCESS/FAILURE/INCONCLUSIVE]

Primary Metric:
- Hypothesis: [CONFIRMED/REJECTED]
- Result: [QUANTITATIVE_OUTCOME]
- Statistical significance: [YES/NO at CONFIDENCE_LEVEL]
- Practical significance: [IS_EFFECT_SIZE_MEANINGFUL]

Example:
"SUCCESS: Variant increased signup conversion from 12% to 14.5% (+2.5pp, +21% relative), p<0.01, 99% confidence. Effect size is both statistically and practically significant."

Secondary Metrics Summary:
- [METRIC_1]: [RESULT_AND_INTERPRETATION]
- [METRIC_2]: [RESULT_AND_INTERPRETATION]
- [METRIC_3]: [RESULT_AND_INTERPRETATION]

Guardrails:
- All within acceptable bounds: [YES/NO]
- Concerns: [ANY_ISSUES]

Unexpected Findings:
- [FINDING_1]: [DESCRIPTION_AND_IMPLICATION]
- [FINDING_2]: [DESCRIPTION_AND_IMPLICATION]

### 10. BUSINESS IMPACT ASSESSMENT

Impact Projection:
If we ship this to 100% of users:
- Primary metric improvement: [EXPECTED_IMPACT]
- Affected users: [COUNT]
- Business value: [REVENUE/COST/OTHER]

Example:
"If we ship to all 1M monthly users:
- Conversion rate: 12% → 14.5%
- Additional conversions: 25,000/month
- Additional revenue: $250K/month ($3M annually)
- ROI: 50:1 (development cost: $60K)"

Cost-Benefit Analysis:
Benefits:
- Quantitative: [REVENUE/CONVERSIONS/TIME_SAVED]
- Qualitative: [USER_EXPERIENCE/BRAND]

Costs:
- Development: [COST_AND_TIME]
- Maintenance: [ONGOING_COST]
- Opportunity cost: [WHAT_WE'RE_NOT_DOING]

ROI: [RETURN_ON_INVESTMENT]

### 11. DECISION & RECOMMENDATIONS

Recommendation: [SHIP/KILL/ITERATE]

Rationale:
- Evidence: [WHAT_DATA_SHOWS]
- Confidence: [HIGH/MEDIUM/LOW]
- Business case: [WHY_THIS_DECISION]
- Risks: [WHAT_COULD_GO_WRONG]

If SHIP:
- Rollout plan: [PHASED/IMMEDIATE]
- Timeline: [SCHEDULE]
- Monitoring plan: [WHAT_TO_WATCH]
- Success criteria: [POST-ROLLOUT_VALIDATION]

If KILL:
- Why: [RATIONALE]
- Learnings: [WHAT_WE_LEARNED]
- Next steps: [ALTERNATIVE_APPROACH]

If ITERATE:
- What to change: [MODIFICATIONS]
- Next experiment: [FOLLOW_UP_TEST]
- Timeline: [WHEN_TO_RETEST]

### 12. LEARNINGS & DOCUMENTATION

Key Learnings:
1. [LEARNING_1]
   - What we learned: [INSIGHT]
   - Why it matters: [IMPLICATION]
   - Future application: [HOW_TO_USE]

2. [LEARNING_2]
   (Same structure)

3. [LEARNING_3]
   (Same structure)

Methodology Learnings:
- What worked well: [PROCESS_WINS]
- What to improve: [PROCESS_GAPS]
- Tools/techniques to reuse: [BEST_PRACTICES]

Audience Insights:
- User behavior patterns: [OBSERVATIONS]
- Segment differences: [VARIATIONS]
- Surprising reactions: [UNEXPECTED_FINDINGS]

Knowledge Share:
- Documentation: [WHERE_STORED]
- Presentation: [TO_WHOM/WHEN]
- Playbook updates: [PROCESS_CHANGES]
- Team retrospective: [SCHEDULED]

### 13. COMMON PITFALLS TO AVOID

Statistical Pitfalls:
- [ ] Not running long enough to reach significance
- [ ] Peeking at results and stopping early
- [ ] Confusing statistical and practical significance
- [ ] Multiple comparison problem (testing many metrics)
- [ ] Sample ratio mismatch (uneven split)
- [ ] Novelty/primacy effects
- [ ] Ignoring external factors (seasonality, campaigns)

Technical Pitfalls:
- [ ] Inconsistent randomization
- [ ] Tracking errors or missing data
- [ ] Performance issues biasing results
- [ ] Cache or cookie issues
- [ ] Mobile vs desktop inconsistencies

Business Pitfalls:
- [ ] Testing too small a change (MDE too small)
- [ ] Testing without clear hypothesis
- [ ] Optimizing local max (wrong metric)
- [ ] Ignoring long-term effects
- [ ] Not considering segment differences
```

## Variables

### PRODUCT_NAME
Your product or feature.
**Examples:**
- "E-commerce checkout flow"
- "Mobile app onboarding"
- "Pricing page"

### HYPOTHESIS
What you believe will happen.
**Examples:**
- "Showing social proof will increase signups by 20%"
- "Simplifying the form from 10 to 5 fields will improve completion rate"
- "Adding urgency messaging will increase conversion"

### TARGET_USERS
Who you're testing with.
**Examples:**
- "All new visitors to signup page"
- "Mobile users in US market"
- "Free tier users considering upgrade"

### PRIMARY_METRIC
Key success measure.
**Examples:**
- "Signup conversion rate"
- "Time to complete onboarding"
- "Purchase conversion rate"

### DECISION
What you'll decide based on results.
**Examples:**
- "Whether to ship new checkout design to all users"
- "Which of 3 headline variations to use"
- "Whether to invest in feature development"

### STATISTICAL_RIGOR
Confidence requirements.
**Examples:**
- "95% confidence, 80% power, 10% MDE"
- "99% confidence for high-risk change"
- "Two-week minimum test duration"

## Usage Examples

### Example 1: Checkout Flow Optimization
```
Hypothesis: Removing optional fields reduces cart abandonment
Test: One-page checkout (control) vs streamlined 3-field checkout (variant)
Primary Metric: Purchase completion rate
Baseline: 45% complete purchase
Sample: 20,000 users (10,000 each)
Duration: 3 weeks
Result: 52% completion (+7pp, +16%), p<0.01
Decision: SHIP - projected $500K additional annual revenue
```

### Example 2: Mobile App Onboarding
```
Hypothesis: Video tutorial increases feature adoption
Test: Text-based onboarding vs 60-second video
Primary Metric: % users who use core feature in first week
Baseline: 35% activation
Sample: 15,000 new users
Duration: 2 weeks
Result: 33% activation (-2pp, -6%), p=0.04
Decision: KILL - video actually hurt activation, likely too long
Learning: Test shorter 15-second version next
```

### Example 3: Pricing Page Test
```
Hypothesis: Showing annual savings increases annual plan adoption
Test: Control (monthly and annual side-by-side) vs variant (annual with "Save $XX/year" badge)
Primary Metric: % choosing annual plan
Baseline: 30% choose annual
Sample: 8,000 visitors
Duration: 10 days
Result: 37% annual (+7pp, +23%), p<0.001
Decision: SHIP - higher LTV, better economics
```

### Example 4: Email Campaign A/B Test
```
Hypothesis: Personalized subject lines increase open rates
Test: Generic subject vs name + personalized content
Primary Metric: Email open rate
Baseline: 22% open rate
Sample: 100,000 emails (50K each)
Duration: 1 day (email campaign)
Result: 26% open rate (+4pp, +18%), p<0.001
Secondary: Click rate 12% → 15% (+3pp, +25%)
Decision: SHIP - use personalization in all campaigns
```

## Best Practices

### Experiment Design
1. **One variable at a time** - Isolate what you're testing
2. **Start with hypothesis** - Not "let's test this" but "we believe X because Y"
3. **Right sample size** - Calculate upfront, don't guess
4. **Run to completion** - No peeking and early stopping
5. **Test what matters** - Primary metric aligned to business goal

### Statistical Rigor
1. **Pre-register** - Document hypothesis and analysis plan before running
2. **Appropriate significance** - 95% for most, 99% for critical changes
3. **Sufficient power** - 80% minimum to detect true effects
4. **Account for multiple tests** - Bonferroni correction if testing many metrics
5. **Check assumptions** - Ensure statistical test requirements met

### Practical Considerations
1. **Balance speed and rigor** - Don't over-index on either
2. **Segment analysis** - Always check if effects vary by segment
3. **Qualitative + quantitative** - Combine numbers with user feedback
4. **Long-term effects** - Consider if short-term gains sustainable
5. **Opportunity cost** - Is this the best use of experiment capacity?

### Organizational
1. **Build experimentation culture** - Everyone can run tests
2. **Shared infrastructure** - Common tools and frameworks
3. **Transparent results** - Share wins and failures
4. **Systematic learnings** - Document and share insights
5. **Continuous testing** - Always be experimenting

## Common Pitfalls

❌ **Testing without hypothesis** - Just trying things randomly
✅ Instead: Clear hypothesis grounded in data or user research

❌ **Sample size too small** - Underpowered tests that can't detect effects
✅ Instead: Calculate required sample size upfront

❌ **Stopping early** - Peeking at results and stopping when significant
✅ Instead: Run to planned sample size or duration

❌ **Ignoring guardrails** - Improving primary metric but hurting quality
✅ Instead: Always monitor guardrail metrics

❌ **Testing too many things** - Multivariate tests without enough traffic
✅ Instead: Simple A/B tests, one variable at a time

❌ **Confusing correlation and causation** - Observational analysis vs true experiment
✅ Instead: Proper randomization and control

❌ **Local optimization** - Optimizing wrong metric
✅ Instead: Ensure primary metric ladders to business goal

❌ **Not documenting learnings** - Same experiments run repeatedly
✅ Instead: Central repository of all experiments and learnings

## Experiment Checklist

Before Launch:
- [ ] Hypothesis clearly stated
- [ ] Primary and secondary metrics defined
- [ ] Sample size calculated
- [ ] Variants built and QA tested
- [ ] Tracking implemented and verified
- [ ] Randomization tested
- [ ] Dashboard created
- [ ] Stakeholder alignment
- [ ] Timeline established

During Experiment:
- [ ] Monitor daily for technical issues
- [ ] Check sample ratio match
- [ ] Verify tracking accuracy
- [ ] Monitor guardrail metrics
- [ ] No early peeking at results
- [ ] Document any incidents

Post-Experiment:
- [ ] Statistical analysis complete
- [ ] Segment analysis complete
- [ ] Business impact calculated
- [ ] Results documented
- [ ] Decision made and communicated
- [ ] Learnings captured
- [ ] Next steps identified

---

**Last Updated:** 2025-11-12
**Category:** Product Management > Product Analytics
**Difficulty:** Intermediate to Advanced
**Estimated Time:** 3-6 weeks per experiment (design to decision)
