---
category: education
related_templates:
- education/Academic-Research/literature-review-protocol-search.md
- education/Academic-Research/literature-review-selection-quality.md
- education/Academic-Research/literature-review-extraction-synthesis.md
- education/Academic-Research/literature-review-analysis-implications.md
- education/Academic-Research/literature-review-reporting-dissemination.md
tags:
- literature-review-readiness
- systematic-review-assessment
- evidence-synthesis-planning
- review-project-evaluation
title: Literature Review Project Readiness Assessment
use_cases:
- Evaluating readiness to conduct a systematic review or evidence synthesis project
- Identifying capability gaps before committing to a literature review
- Assessing team capacity and resource requirements for review projects
- Planning literature review timelines and milestones with risk mitigation
industries:
- education
- government
- healthcare
- technology
type: framework
difficulty: intermediate
slug: literature-reviews-overview
---

# Literature Review Project Readiness Assessment

## Purpose
Comprehensively assess your readiness to conduct a rigorous systematic review, meta-analysis, or evidence synthesis project across six critical dimensions: Methodological Expertise, Search & Selection Capability, Synthesis & Analysis Readiness, Infrastructure & Tools, Team & Resources, and Reporting & Dissemination Capacity. This framework identifies gaps, estimates timelines, and creates a project execution plan before committing to a multi-month review.

## ðŸš€ Quick Assessment Prompt

> Assess **literature review project readiness** for **{REVIEW_CONTEXT}** planning to conduct **{REVIEW_TYPE}**. Evaluate across: (1) **Methodological expertise**â€”what systematic review training exists? Experience with PRISMA, protocol registration, quality assessment tools? (2) **Search capability**â€”can the team design comprehensive searches across databases? Access to information specialists? (3) **Synthesis capacity**â€”what statistical and qualitative synthesis skills exist? Software proficiency (RevMan, R, NVivo)? (4) **Infrastructure**â€”what screening tools, reference management, and collaboration platforms are available? (5) **Team resources**â€”what time commitment, dual-reviewer capacity, and content expertise exists? (6) **Dissemination readiness**â€”what publication targets, stakeholder engagement plans, and knowledge translation capacity exists? Provide a maturity scorecard (1-5 per dimension), risk analysis, resource requirements, and a phased project plan with realistic timeline.

**Usage:** Replace bracketed placeholders with your specifics. Use as a prompt to an AI assistant for rapid literature review project readiness evaluation.

---

## Template

Conduct a comprehensive literature review project readiness assessment for {REVIEW_CONTEXT}, a {REVIEW_TYPE} targeting {TARGET_OUTCOMES}.

Assess readiness across six dimensions, scoring each 1-5:

**1. METHODOLOGICAL EXPERTISE & TRAINING READINESS**
Evaluate whether the team possesses foundational systematic review methodology knowledge by assessing formal training in evidence synthesis methods, familiarity with reporting guidelines such as PRISMA or JBI standards, understanding of protocol development and registration processes through platforms like PROSPERO or OSF, experience with quality assessment frameworks including Cochrane Risk of Bias tools or Newcastle-Ottawa Scale, knowledge of different review types and their methodological requirements such as distinguishing systematic reviews from scoping reviews or realist syntheses, and awareness of ethical considerations including conflict of interest management and transparent reporting practices that ensure methodological rigor throughout the review lifecycle.

**2. SEARCH STRATEGY & SELECTION CAPABILITY READINESS**
Evaluate whether the team can design and execute comprehensive literature searches by assessing ability to formulate answerable questions using PICO or similar frameworks, capacity to design multi-database search strategies with appropriate Boolean operators and controlled vocabulary, access to information specialists or librarians with systematic review expertise, familiarity with grey literature sources and supplementary search methods including hand-searching and citation chasing, proficiency with reference management systems such as EndNote or Zotero for deduplication and organization, experience with dual independent screening workflows and inter-rater reliability calculation, and capacity to document search decisions and create PRISMA flow diagrams that demonstrate transparent study selection processes.

**3. SYNTHESIS & ANALYSIS CAPABILITY READINESS**
Evaluate whether the team can conduct defensible evidence synthesis by assessing statistical expertise for meta-analysis including effect size calculation and heterogeneity assessment, proficiency with meta-analysis software such as RevMan, R metafor package, or Stata, qualitative synthesis skills including thematic analysis and framework synthesis methods, experience with data extraction form design and dual extraction with verification processes, capacity to assess publication bias through funnel plots or statistical tests, ability to conduct subgroup analyses and meta-regression when appropriate, mixed-methods integration capability for reviews combining quantitative and qualitative evidence, and understanding of when narrative synthesis is more appropriate than statistical pooling based on heterogeneity and data characteristics.

**4. INFRASTRUCTURE, TOOLS & WORKFLOW READINESS**
Evaluate whether adequate tools and systems support efficient review execution by assessing access to screening management platforms such as Covidence, Rayyan, or DistillerSR, availability of statistical software for quantitative synthesis and analysis, access to qualitative analysis software like NVivo or MAXQDA if needed, secure data storage and version control systems compliant with research data management requirements, collaboration platforms for team communication and task coordination such as Slack or Microsoft Teams, institutional database subscriptions and access to full-text articles, capacity for protocol registration and data sharing through repositories, and established workflows for quality control including audit trails and discrepancy resolution processes that ensure reproducibility and transparency.

**5. TEAM COMPOSITION, TIME & RESOURCE READINESS**
Evaluate whether the project has sufficient human and financial resources by assessing core team size with minimum two independent reviewers for screening and quality assessment, content expertise alignment with the review question domain, realistic time commitment estimation with typical systematic reviews requiring 300-1000+ person-hours depending on scope, availability throughout the project timeline spanning 6-18 months for complete reviews, budget allocation for database access, software licenses, publication fees and potential professional editing, access to methodological consultation or mentorship for complex synthesis decisions, stakeholder engagement capacity for co-production models when developing practice or policy guidance, and contingency planning for personnel changes or scope adjustments that may arise during long-duration projects.

**6. REPORTING, DISSEMINATION & IMPACT READINESS**
Evaluate whether the project can effectively communicate findings to target audiences by assessing publication strategy alignment with journal scope and impact including pre-identification of target journals, stakeholder identification and engagement planning for knowledge translation, capacity to produce multiple dissemination products including manuscripts, policy briefs, and plain language summaries tailored to diverse audiences, experience with reporting guideline compliance such as completing PRISMA checklists and providing supplementary materials, open science commitment including data sharing and protocol transparency, knowledge translation planning for implementation support beyond passive dissemination, impact measurement framework for tracking uptake and influence, and sustainability planning for review updates or living systematic reviews that maintain currency as new evidence emerges.

---

## Required Output Format

Structure your assessment as:

1. **EXECUTIVE SUMMARY** - Overall readiness score (X.X/5.0), maturity level, go/no-go recommendation, top 3 risks, estimated timeline (X months)

2. **DIMENSION SCORECARD** - Table with dimension name, score (X.X/5), key strength, key gap, priority action

3. **FEASIBILITY ANALYSIS** - For the proposed {REVIEW_TYPE}, assess: scope appropriateness (too broad/narrow?), volume estimate (expected studies to screen/include), timeline realism (based on team capacity), resource sufficiency (gaps in access, tools, expertise)

4. **RISK REGISTER** - Top 5 project risks ranked by likelihood and impact, with mitigation strategies and contingency plans

5. **CAPABILITY BUILDING PLAN** - Prioritized actions to address gaps: training needs (courses, workshops, mentorship), tool acquisition (software, database access, collaboration platforms), team expansion (additional reviewers, specialists, consultants), process development (workflows, templates, quality control procedures)

6. **PROJECT ROADMAP** - Phased timeline with: Protocol Development (weeks X-Y), Search & Screening (weeks Y-Z), Data Extraction (weeks Z-A), Synthesis & Analysis (weeks A-B), Manuscript Preparation (weeks B-C), milestones and decision points

Use this maturity scale:
- **1.0-1.9 (Exploratory)**: Minimal systematic review experience, significant training gaps, limited tools, feasibility uncertain
- **2.0-2.9 (Developing)**: Some methodology knowledge, basic tools available, team gaps exist, supervision recommended
- **3.0-3.9 (Capable)**: Solid systematic review foundation, adequate tools and team, can execute with quality assurance
- **4.0-4.9 (Proficient)**: Extensive review experience, comprehensive infrastructure, efficient workflows, mentorship-ready
- **5.0 (Expert)**: Leading systematic review expertise, advanced methods, optimized systems, field contribution capacity

---

## Variables

| Variable | Description | Example |
|----------|-------------|---------|
| `{REVIEW_CONTEXT}` | The organization, research team, or individual conducting the review and domain area | "Graduate student health sciences program investigating telemedicine effectiveness" |
| `{REVIEW_TYPE}` | The specific type of literature review or evidence synthesis method planned | "systematic review with meta-analysis" or "scoping review" or "qualitative evidence synthesis" |
| `{TARGET_OUTCOMES}` | The primary questions, outcomes, or objectives the review aims to address | "clinical effectiveness and cost-effectiveness of remote patient monitoring for chronic disease management" |

---

## Usage Example

**Input:**
```
REVIEW_CONTEXT: Doctoral candidate in educational psychology with dissertation committee, investigating digital learning interventions
REVIEW_TYPE: Systematic review with meta-analysis of randomized controlled trials
TARGET_OUTCOMES: Effect of gamification on student engagement and learning outcomes in higher education
```

**Output:**
```
EXECUTIVE SUMMARY
Overall Readiness: 2.4/5.0 (Developing)
Maturity Level: Developing - Project feasible with structured support
Recommendation: CONDITIONAL GO - Proceed with enhanced supervision and capability building
Top 3 Risks: (1) No information specialist access - search quality risk, (2) Limited meta-analysis experience - synthesis capacity gap, (3) Single researcher model - screening efficiency and bias concerns
Estimated Timeline: 10-14 months with current capacity

DIMENSION SCORECARD
| Dimension | Score | Strength | Gap | Priority Action |
|-----------|-------|----------|-----|-----------------|
| Methodological Expertise | 2.5/5.0 | Completed Cochrane training module | No hands-on review experience | Engage systematic review mentor |
| Search Capability | 2.0/5.0 | Database access via university | No information specialist collaboration | Schedule librarian consultation |
| Synthesis Capability | 2.3/5.0 | Strong quantitative background (stats coursework) | No RevMan or R meta-analysis experience | Complete R metafor tutorial, practice dataset |
| Infrastructure & Tools | 3.0/5.0 | University Covidence license available | No qualitative software if needed | Adequate for quantitative review |
| Team & Resources | 1.8/5.0 | Dissertation committee oversight | Single primary reviewer (no dual screening) | Recruit second reviewer (peer or RA) |
| Dissemination Readiness | 2.5/5.0 | Clear publication target (dissertation + journal) | No stakeholder engagement plan | Define practitioner dissemination strategy |

FEASIBILITY ANALYSIS
- Scope: Appropriate for dissertation systematic review, question well-bounded by intervention type (gamification) and population (higher education)
- Volume Estimate: Expect 2,000-4,000 titles/abstracts to screen â†’ 200-400 full texts â†’ 20-40 included RCTs based on preliminary scoping
- Timeline Realism: 10-14 months realistic IF second reviewer secured and meta-analysis mentorship available; single-reviewer model would extend to 16-20 months with quality risks
- Resource Sufficiency: Database access adequate; Covidence license secures screening workflow; gap in meta-analysis software (need R or RevMan training); no budget identified for potential publication fees (~$2,000-3,000 open access)

RISK REGISTER
1. **Search Quality Risk** (Likelihood: High, Impact: High) - No information specialist collaboration risks incomplete database coverage and suboptimal search strings â†’ Mitigation: Schedule consultation with education/health sciences librarian, pilot search with feedback, document search peer review
2. **Single-Reviewer Bias** (Likelihood: High, Impact: Medium) - Solo screening/extraction increases selection bias and error rate â†’ Mitigation: Recruit second reviewer (fellow doctoral student reciprocal arrangement), implement 20% dual extraction for quality check, use Covidence conflict resolution features
3. **Meta-Analysis Skill Gap** (Likelihood: Medium, Impact: High) - Limited hands-on experience risks methodological errors in pooling and heterogeneity assessment â†’ Mitigation: Complete R meta-analysis course (Coursera/Cochrane), work through practice dataset with mentor feedback before actual analysis, consider statistician consultation for complex scenarios
4. **Timeline Pressure** (Likelihood: Medium, Impact: Medium) - Dissertation deadlines may compress quality assurance â†’ Mitigation: Build 2-month buffer into timeline, define minimum viable scope (could reduce to narrative synthesis if meta-analysis timeline threatens graduation), monthly milestone check-ins with committee
5. **Scope Creep** (Likelihood: Medium, Impact: Medium) - Broad gamification definition could expand included studies unmanageably â†’ Mitigation: Finalize operational definition in protocol, pilot eligibility criteria on 50 abstracts with committee review, register protocol to commit to scope boundaries

CAPABILITY BUILDING PLAN (Pre-Launch, 6-8 weeks)
Training Priorities:
- Week 1-2: Complete Cochrane Interactive Learning Module 1-4 (systematic review basics)
- Week 3-4: R meta-analysis tutorial (metafor package, effect size calculation, forest plots)
- Week 5: Covidence training webinar and practice dataset
- Ongoing: Join Cochrane/Campbell methods webinar series

Tool Acquisition:
- Week 1: Activate Covidence license through university library
- Week 2: Install R/RStudio with metafor and meta packages
- Week 3: Set up Zotero or EndNote for reference management
- Week 4: Establish shared Google Drive folder structure for team collaboration

Team Expansion:
- Week 2-3: Recruit second reviewer (post on doctoral student listserv, propose reciprocal review arrangement)
- Week 4: Engage education librarian for search strategy consultation (2-3 sessions)
- Week 6: Identify statistician mentor for meta-analysis consultation (committee member or methodologist)

Process Development:
- Week 5: Develop screening and extraction forms with committee input
- Week 6: Create quality assessment workflow (RoB 2.0 training, reliability pilot)
- Week 7: Establish discrepancy resolution protocol and documentation standards
- Week 8: Finalize protocol and register on PROSPERO before search execution

PROJECT ROADMAP (48-week dissertation timeline)
Phase 1: Protocol Development (Weeks 1-8)
- Capability building activities (training, recruitment, tool setup)
- PICO refinement and search strategy with librarian
- Protocol drafting with committee feedback (2 revision cycles)
- PROSPERO registration and ethics review if needed
Milestone: Approved registered protocol

Phase 2: Search & Screening (Weeks 9-22, 14 weeks)
- Database searches and deduplication (weeks 9-10)
- Title/abstract screening with second reviewer (weeks 11-16, ~300 abstracts/week)
- Full-text review and eligibility assessment (weeks 17-21, ~30 texts/week)
- Contact authors for missing data if needed (week 22)
Milestone: Final included study set with PRISMA diagram

Phase 3: Data Extraction & Quality Assessment (Weeks 23-32, 10 weeks)
- Extraction form pilot and refinement (week 23)
- Dual extraction with verification (weeks 24-30, ~5 studies/week)
- Risk of bias assessment (weeks 28-32, parallel with extraction)
- Data synthesis preparation and cleaning (week 32)
Milestone: Complete extraction database and quality ratings

Phase 4: Synthesis & Analysis (Weeks 33-40, 8 weeks)
- Meta-analysis conduct in R (effect size pooling, heterogeneity assessment) (weeks 33-36)
- Subgroup and sensitivity analyses (weeks 37-38)
- Publication bias assessment and GRADE ratings (week 39)
- Results interpretation with mentor review (week 40)
Milestone: Complete analysis with figures and tables

Phase 5: Manuscript Preparation (Weeks 41-48, 8 weeks)
- Dissertation chapter drafting (weeks 41-44)
- Committee review and revision (weeks 45-46)
- PRISMA checklist completion and supplementary materials (week 47)
- Final dissertation submission and journal article adaptation (week 48)
Milestone: Dissertation chapter accepted and journal manuscript submitted

SUCCESS METRICS
- Protocol registered on PROSPERO by Week 8 (transparency)
- Inter-rater reliability Îº > 0.80 for screening (quality control)
- 100% dual extraction with <5% discrepancy rate (accuracy)
- Meta-analysis heterogeneity IÂ² documented and explored (rigor)
- PRISMA checklist 100% complete (reporting quality)
- Dissertation defense passed (primary outcome)
- Journal manuscript submitted within 6 months post-defense (dissemination)
```

**Key Findings:** Doctoral candidate has foundational knowledge but critical gaps in dual-reviewer capacity and hands-on meta-analysis experience. Project is feasible with structured capability building (8-week pre-launch phase) and enhanced supervision. Conditional recommendation emphasizes recruiting second reviewer (highest priority) and securing meta-analysis mentorship before launch. Timeline of 48 weeks is realistic with these supports in place. Search strategy consultation with librarian is essential to avoid critical coverage gaps.

---

## Related Resources

### Specialized Module Templates

This overview assessment identifies overall project readiness. For detailed phase-specific assessments, use:

- **[Literature Review Protocol & Search Strategy Readiness Assessment](literature-review-protocol-search.md)** - Deep assessment of protocol development, research question formulation, search strategy design, and registration readiness
- **[Literature Review Selection & Quality Assessment Readiness Assessment](literature-review-selection-quality.md)** - Detailed evaluation of screening workflow, eligibility operationalization, dual review processes, and quality assessment tool application
- **[Literature Review Data Extraction & Synthesis Readiness Assessment](literature-review-extraction-synthesis.md)** - Comprehensive assessment of extraction capability, quantitative meta-analysis readiness, qualitative synthesis skills, and mixed-methods integration
- **[Literature Review Analysis & Implications Readiness Assessment](literature-review-analysis-implications.md)** - Evaluation of evidence grading frameworks, theoretical contribution capability, practice/policy recommendation development, and research gap identification
- **[Literature Review Reporting & Dissemination Readiness Assessment](literature-review-reporting-dissemination.md)** - Assessment of manuscript preparation, reporting standard compliance, stakeholder communication planning, and knowledge translation capacity

### Workflow Sequence

1. **Start here** with this overview assessment to evaluate overall project feasibility
2. If overall readiness score â‰¥ 2.5/5.0, proceed to **Module 1 (Protocol & Search)** for detailed planning
3. Progress sequentially through **Modules 2-5** as review execution advances
4. Return to module-specific assessments when capability gaps emerge during execution

### Explore More in This Category

Browse all **[education/Academic Research](../Academic-Research/)** templates for related research methodology tools and frameworks.
