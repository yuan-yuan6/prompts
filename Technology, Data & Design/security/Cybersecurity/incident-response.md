---
category: security
title: Incident Response Framework
tags:
- security
- incident-response
- playbooks
- containment
use_cases:
- Building incident response capability with NIST 800-61 framework covering preparation, detection, containment, eradication, recovery, post-incident achieving MTTD <1h, MTTR <4h
- Creating incident playbooks for ransomware, data breach, phishing, DDoS with response procedures, escalation paths, communication templates
- Conducting incident response exercises (tabletop, red team, purple team) validating detection, response, recovery capabilities quarterly
related_templates:
- security/Cybersecurity/security-operations.md
- security/Security-Operations/siem-security-monitoring.md
- security/Cybersecurity/threat-intelligence.md
industries:
- technology
- financial-services
- healthcare
- government
type: framework
difficulty: intermediate
slug: incident-response
---

# Incident Response Framework

## Purpose
Build comprehensive incident response capability covering preparation, detection and analysis, containment, eradication, recovery, and post-incident activities achieving rapid response (MTTD <1 hour, MTTR <4 hours) and regulatory compliance (GDPR 72-hour notification, HIPAA breach notification).

## ðŸš€ Quick Incident Response Prompt

> Build incident response for **[ORGANIZATION]**. Team: **[COMMANDER]**, **[SECURITY]**, **[IT_OPS]**, **[LEGAL]**, **[COMMS]**. Classification: **[P1_CRITICAL/P2_HIGH/P3_MEDIUM/P4_LOW]**. Playbooks: **[RANSOMWARE/DATA_BREACH/PHISHING/DDOS/INSIDER]**. Tools: **[SIEM]** + **[EDR]** + forensics **[TOOLKIT]**. Exercises: quarterly tabletop, annual red team. Compliance: **[GDPR/HIPAA/PCI]** notification. Deliver: IR plan, playbooks, runbooks, communication templates.

---

## Template

Build incident response capability for {ORGANIZATION} handling {INCIDENT_TYPES} achieving {RESPONSE_SLAS} with {COMPLIANCE_REQUIREMENTS} regulatory compliance.

**INCIDENT RESPONSE PREPARATION**

Establish IR capability before incidents occur. IR team structure: incident commander (CISO or Security Manager leads response, authority to make critical decisionsâ€”isolate systems, engage external IR firm, coordinate with stakeholders, on-call rotation with backup), security analysts (tier 1 triage alerts and initial investigation, tier 2 deep-dive analysis and containment, tier 3 forensics and malware analysis), IT operations (system admins for containment and recovery, network team for isolation and segmentation, database admins for data recovery, cloud ops for cloud incidents), legal counsel (breach notification requirements, regulatory reporting, law enforcement coordination, litigation hold procedures), communications (internal comms to employees, external to customers/partners/media, PR for reputation management, executive briefings), external resources (IR retainer with CrowdStrike/Mandiant/Kroll, cyber insurance with IR coverage, FBI cyber liaison, forensics lab for evidence analysis).

IR tools and technology: SIEM for detection (Splunk, Sentinel, Elastic aggregates logs from all sources, correlation rules detect attack patterns, real-time alerting on critical threats, threat intelligence integration), EDR for endpoint visibility (CrowdStrike, Defender, SentinelOne provides visibility into endpoint activity, remote isolation capability critical, memory and disk forensics built-in, automated response actions), forensic toolkit (forensic workstation with write-blockers, FTK Imager for disk imaging, Volatility for memory analysis, Wireshark for network packet analysis, SIFT Workstation or REMnux for malware analysis), communication platform (dedicated Slack/Teams channel for IR coordination, conference bridge for war room calls, secure file sharing for evidence and reports, status page for customer communication), documentation (incident tracking in JIRA/ServiceNow, runbooks in Confluence/SharePoint, evidence repository with chain of custody, timeline documentation tool).

IR procedures and playbooks: incident classification (P1 Criticalâ€”active data breach, ransomware, production outage, <15 min response, executive notification immediate, P2 Highâ€”confirmed malware, privilege escalation, <1 hour response, CISO notification, P3 Mediumâ€”suspicious activity, policy violation, <4 hour response, manager notification, P4 Lowâ€”informational, false positive, best effort response), incident playbooks by type (ransomware: isolate, collect ransom note and IOCs, assess backup viability, engage legal/insurance, restore from clean backups, data breach: scope determination, regulatory notification timelinesâ€”GDPR 72 hours, evidence preservation, customer notification, phishing: user education, credential reset, block sender/URL, hunt for lateral movement, DDoS: traffic scrubbing via ISP/CDN, rate limiting, communication to customers, insider threat: HR coordination, discrete monitoring, preserve evidence, legal considerations), escalation procedures (when to escalate: incident severity increases, containment unsuccessful after 2 hours, regulatory notification threshold reached, media inquiry, executive involvement required, escalation path: analyst â†’ team lead â†’ SOC manager â†’ CISO â†’ CEO â†’ board).

**DETECTION AND ANALYSIS**

Identify and scope security incidents. Detection sources: automated detection (SIEM correlation rules fire on attack patternsâ€”brute force, data exfiltration, lateral movement, EDR behavioral detectionâ€”malware, ransomware, privilege escalation, IDS/IPS signatures for network-based attacks, DLP for sensitive data movement, threat intelligence IOC matching), manual detection (user reports of suspicious activity, security analyst threat hunting, vendor notifications of compromise, law enforcement alert), third-party notification (customer reports unauthorized access, external researcher responsible disclosure, threat intelligence firm warning). Initial triage: alert validation (is this true positive or false positive?, correlation with other alerts and logs, threat intel lookup on IOCs, user contextâ€”is behavior anomalous for this user?), severity assessment (what systems affected?, what data at risk?, business impactâ€”revenue, operations, reputation, compliance implicationsâ€”breach notification required?), evidence collection (preserve volatile data firstâ€”memory, running processes, collect logs before retention expiration, screenshot everything, maintain chain of custody).

Forensic investigation: forensic principles (order of volatilityâ€”memory first then disk, avoid modifying evidence, hash everything for integrity, maintain detailed notes with timestamps, use forensically sound toolsâ€”write-blockers for disk access), memory forensics (capture memory with FTK Imager or DumpIt, analyze with Volatility for malware artifacts, running processes, network connections, extract passwords, encryption keys, malware from memory), disk forensics (create forensic image with write-blocker, mount read-only for analysis, file timeline analysisâ€”when were files created/modified/accessed, file carving for deleted files, registry analysis for persistence mechanisms, browser history and artifacts), network forensics (full packet capture if availableâ€”PCAP analysis, NetFlow/sFlow for traffic analysis, DNS logs for C2 communication, proxy logs for data exfiltration, timeline correlation across network and endpoint), malware analysis (static analysisâ€”strings, PE headers, packing detection, dynamic analysis in sandboxâ€”Cuckoo, ANY.RUN, behavioral analysisâ€”network, file, registry activity, YARA rules for detection, reverse engineering if needed).

Attack timeline and scope: timeline reconstruction (establish initial access timeâ€”when did attacker first compromise?, document lateral movementâ€”what systems were accessed and when?, identify data exfiltrationâ€”what was stolen and when?, create visual timeline for executive communication), scope assessment (how many systems compromised?, identify patient zeroâ€”first infected system, map attacker movement through environment, assess data accessâ€”what sensitive data could attacker have accessed?, cloud scopeâ€”any cloud resources affected?), attribution analysis (identify adversary TTPsâ€”map to MITRE ATT&CK, infrastructure analysisâ€”C2 domains and IPs, malware analysis for attribution indicators, compare to known threat actor TTPs, confidence levelâ€”low/medium/high), root cause analysis (how did attacker gain initial access?â€”phishing, vulnerability exploitation, stolen credentials, what allowed lateral movement?â€”flat network, weak passwords, privileged accounts, what enabled data access?â€”overly permissive access controls, unencrypted data).

**CONTAINMENT STRATEGY**

Limit incident impact while preserving evidence. Short-term containment: immediate actions to prevent spread (isolate affected systems via EDR network containment, block malicious IPs/domains at firewall and DNS, disable compromised user accounts, revoke access tokens and API keys, increase monitoring on critical systems, snapshot systems before changes for forensics), containment decisions (balance business continuity vs securityâ€”can we keep production running while containing?, preserve evidenceâ€”don't destroy logs or artifacts, communicationâ€”notify stakeholders of service disruptions, legal considerationsâ€”consult legal before taking systems offline that may have evidence). Long-term containment: sustained measures while preparing recovery (rebuild compromised systems from known-good images, reset all credentialsâ€”passwords, API keys, certificates, secrets, patch vulnerabilities exploited in attack, implement additional monitoring for re-infection, network segmentation to prevent future lateral movement, enhanced logging on critical systems).

Containment by incident type: ransomware containment (immediate isolation of infected systemsâ€”pull network cables if needed to stop spread, identify and protect clean backupsâ€”ensure no access from compromised accounts, block C2 communication at firewall/DNS, disable user access to prevent re-infection via phishing, assess decryption vs restore from backupâ€”consult with IR firm and legal, do not pay ransom without legal and executive approval), data breach containment (identify data exfiltration path and block, revoke access for compromised accounts/credentials, implement enhanced monitoring on sensitive data stores, preserve evidence of unauthorized accessâ€”don't purge logs, notify legal for regulatory breach notification assessment, customer communication planning), DDoS containment (activate DDoS mitigationâ€”CDN, scrubbing service, rate limiting at application and infrastructure layers, geo-blocking if attack from specific regions, communication to customers via status pageâ€”avoid email which may also be impacted).

Evidence preservation during containment: forensic best practices (document state before changesâ€”screenshots, photos, video, preserve memory before system shutdown if possible, maintain chain of custody for all evidence, hash all collected evidence for integrity verification, store evidence securelyâ€”encrypted, access-controlled, maintain forensic notesâ€”who did what when and why), legal hold (implement litigation hold if lawsuit anticipated, preserve all relevant evidenceâ€”don't auto-delete logs, notify all custodians of hold requirements, coordinate with legal on scope and duration).

**ERADICATION AND RECOVERY**

Remove threats and restore operations. Eradication: threat removal (malware removalâ€”AV scan after isolation, manual removal for persistent threats, remove persistence mechanismsâ€”scheduled tasks, registry keys, WMI subscriptions, services, patch vulnerabilities exploited by attacker, close security gaps that enabled attack, reset credentials for all potentially compromised accountsâ€”not just known compromised), validation (verify threat completely removedâ€”EDR and SIEM monitoring for 72 hours post-eradication, re-scan systems for IOCs, monitor for re-infection signs, independent validation by forensics team).

Recovery planning: prioritize recovery order (1: authentication systemsâ€”AD, SSO without these nothing else works, 2: communicationâ€”email, collaboration tools, 3: revenue-critical systemsâ€”e-commerce, payment processing, 4: business operationsâ€”CRM, ERP, 5: user workstationsâ€”as needed), recovery timeline (estimate RTO for each system tier, coordinate with business on acceptable downtime, communicate timeline to stakeholders, parallel recovery where possible to reduce total time), recovery from clean backups (verify backups unaffected by incidentâ€”restore point before initial compromise, test restore in isolated environment first, validate data integrity after restore, incremental restore approachâ€”core systems first, monitor for issues during restoration).

System restoration: rebuild vs restore (rebuild from gold imageâ€”stronger assurance of clean state, longer timeline, restore from backupâ€”faster but risk of restoring compromised state, decision factorsâ€”confidence in backup integrity, business urgency, compliance requirements), hardening before restoration (apply all security patches before bringing online, change default passwords and keys, disable unnecessary services and ports, implement compensating controls for residual risks, enhanced monitoring for restored systems), service validation (functionality testing before production, performance testing under load, security testingâ€”vulnerability scan, pentesting if time allows, user acceptance testing, phased rollout to limit impact if issues).

Return to normal operations: declare incident closed (all affected systems recovered, threat eradicated and validated, monitoring shows no re-infection, normal business operations resumed), enhanced monitoring post-incident (continue elevated monitoring for 30 days, watch for IOCs associated with incident, monitor for attacker returnâ€”persistence mechanisms missed, threat hunting based on incident TTPs), process improvement (update detection rules based on incident, patch additional systems proactively, implement security controls to prevent recurrence).

**POST-INCIDENT ACTIVITIES**

Learn and improve from incidents. Post-incident review: timeline (conduct review within 72 hours while details fresh, separate from recovery activities, all stakeholders present, blameless cultureâ€”focus on process not individuals), review structure (what happenedâ€”incident timeline and scope, what worked wellâ€”successful detections and responses, what could improveâ€”gaps and failures, action itemsâ€”specific improvements with owners and due dates), documentation (incident reportâ€”executive summary, technical details, timeline, impact assessment, lessons learned, share findings with broader team, maintain incident archive for future reference).

Metrics and reporting: incident metrics (MTTDâ€”mean time to detect, target <1 hour for critical, MTTRâ€”mean time to respond and recover, target <4 hours for critical, incident count and trend over time, incident by type and severity, false positive rateâ€”target <20%, automation effectivenessâ€”% of tier-1 tasks automated), executive reporting (monthly incident summary to executive team, quarterly metrics review with board, annual program assessment, major incident post-mortem to leadership), regulatory reporting (GDPR breach notification to supervisory authority within 72 hours, HIPAA breach notification to HHSâ€”annual or immediate if >500 records, PCI-DSS incident reporting to acquiring bank and card brands, SEC 8-K if material impact to publicly traded company).

Continuous improvement: detective control tuning (update SIEM correlation rules based on incident TTPs, create new detection for attacker techniques, reduce false positives from lessons learned, integrate incident IOCs into threat intelligence), preventive control enhancement (patch additional systems proactively, implement security controls to prevent similar attacks, update security policies and standards, enhance security training based on gapsâ€”phishing if successful social engineering), procedure updates (update IR playbooks with lessons learned, revise escalation procedures if gaps, improve communication templates, update contact lists and on-call rotation), tabletop exercises (quarterly IR tabletop exercises, scenarios based on actual incidents and threat landscape, test updates to procedures, measure improvement in response time and coordination).

Deliver incident response program as:

1. **IR PLAN** - Comprehensive plan aligned to NIST 800-61, team structure, escalation procedures, regulatory requirements

2. **INCIDENT PLAYBOOKS** - Playbooks per incident type (ransomware, data breach, phishing, DDoS, insider threat, malware) with step-by-step procedures

3. **RUNBOOKS** - Technical runbooks for containment, evidence collection, forensics, recovery

4. **COMMUNICATION TEMPLATES** - Templates for internal, external, regulatory, customer, media communications

5. **FORENSIC PROCEDURES** - Evidence handling, chain of custody, forensic analysis workflows

6. **TRAINING MATERIALS** - IR team training, tabletop exercise scenarios, user awareness

7. **METRICS FRAMEWORK** - KPI definitions, dashboard, reporting templates

---

## Usage Examples

### Example 1: SaaS Company Ransomware Response
**Prompt:** Execute ransomware incident response for CloudApp (500K customers, AWS infrastructure, REvil ransomware, $2M ransom demand) achieving 24-hour recovery.

**Expected Output:** Detection: CrowdStrike EDR alerted on ransomware behavior 3:47 AM (file encryption, volume shadow copy deletion, ransom note creation), SIEM correlated across 12 infected servers, MTTD 18 minutes from initial encryption to SOC escalation. Classification: P1 Criticalâ€”active ransomware, production database servers affected, customer service impacted immediately. Team activation: Incident commander (CISO) paged 4:05 AM, war room established on Zoom + dedicated Slack channel #inc-ransomware-2025-01, team assembledâ€”5 security analysts, 3 cloud ops, 2 DBAs, legal counsel, CEO notified. Containment (4:05-5:30 AM): Isolated 12 infected servers via AWS security groups (network isolation), identified clean backups from 2 hours priorâ€”before initial infection, blocked REvil C2 domains at Route53 DNS, disabled compromised service account, snapshot infected instances for forensics before termination. Analysis (parallel to containment): Forensics collected memory dumps and disk images, malware analysis identified REvil variant targeting AWS, initial access via phishing 3 days priorâ€”developer clicked malicious link, installed info-stealer, credentials exfiltrated, attacker used stolen AWS access key for lateral movement. Ransom demand: $2M Bitcoin, 48-hour deadline, decision matrixâ€”restore from backup vs pay, legal counsel advises against payment (illegal in some jurisdictions, no guarantee of decryption, funds terrorism), insurance contacted but doesn't cover ransom, executive decision: restore from backups. Recovery (5:30 AM - 2:00 PM): Eradicationâ€”terminated infected instances, rotated all AWS credentials and keys, patched vulnerability used for lateral movement, rebuilt 12 database servers from AMIs, restored data from RDS automated backups (2 hours data loss acceptable to business), parallel recovery of application servers, phased restorationâ€”internal testing â†’ beta customers â†’ full production. Validation: Security scan shows no remaining IOCs, monitoring for 48 hours shows no re-infection, functionality testing confirms all services operational, data integrity validation sampling 10K transactions. Communication: Internalâ€”all-hands email explaining incident and 2-hour data loss, customer notificationâ€”email to all 500K customers within 8 hours (transparency builds trust), regulatoryâ€”GDPR notification not required (no data breach, just encryption), mediaâ€”proactive PR statement emphasizing rapid recovery and no data loss. Post-incident: Total downtime 10 hours 15 minutes (better than 24-hour RTO), total data loss 2 hours (better than 4-hour RPO), cost $180K (staff overtime + AWS compute for recovery + forensics), ransom NOT paid saving $2M, post-mortem identified 3 major gapsâ€”MFA not enforced on AWS console (fixed within 24 hours), insufficient phishing training (quarterly training implemented), backup testing not regular (monthly restore drills instituted). Lessons learned: Detection worked (18 min MTTD excellent), backups saved incident (could have been $2M+ ransom + unknown recovery time), communication was effective (customer feedback positive on transparency), improvementsâ€”MFA everywhere, better training, regular backup testing. Outcome: Avoided ransom payment, limited customer impact, strengthened security posture, used incident for security awareness training showing real impact.

### Example 2: Healthcare Data Breach (HIPAA)
**Prompt:** Execute data breach response for HealthSystem (8 hospitals, Epic EHR, unauthorized PHI access, 50K patient records, HIPAA breach notification) achieving 60-day notification.

**Expected Output:** Detection: Epic audit log review flagged unusual patternâ€”terminated employee's credentials accessed 47 patient records over 3 days, SIEM rule detected access from unusual geolocation (employee in California, access from Florida). Classification: P2 Highâ€”confirmed unauthorized PHI access, not ransomware or active attack, but regulatory implications significant, HIPAA breach assessment required. Team: Privacy Officer leads (HIPAA role), Security Manager (forensics), Legal Counsel (breach determination and notification), Epic team (audit log analysis), Communications (patient notification). Investigation: Complete Epic audit log analysisâ€”account accessed 2,847 patients over 2 weeks (much larger than initial 47), geolocation analysis shows access from 3 Florida IPs (employee moved but kept credentials), data accessed: names, DOB, diagnoses, medications, treatment notes (full PHI), no evidence of data exfiltrationâ€”viewing only not downloading (but cannot rule out screenshots), timeline: employee terminated Jan 1, unauthorized access Jan 5-19 (2 weeks before detection), access method: VPN credentials not revoked at termination (process gap). Breach determination: Legal counsel assessment using HIPAA "low probability" standardâ€”unauthorized access to 2,847 patient PHI records, risk to individuals medium (terminated employee, unclear motive, no evidence of identity theft), exceeds 500 individual threshold â†’ breach notification REQUIRED (not just incident). Containment: Revoke VPN credentials immediately (should have been done at termination), force password reset for all Epic accounts as precaution, enhanced monitoring on Epic access logs (real-time alerting for anomalies), review termination processâ€”103 other terminated employees still had active credentials (systemic issue). Regulatory notification: HHS OCR notification submitted within 60 days via web portal (required for >500 individuals), state Attorney General notification per state law, annual breach report to HHS if had been <500 individuals. Individual notification: 2,847 patients notified by first-class mail within 60 days (HIPAA requirement), notification includes: what happened, what PHI was involved, what we're doing to prevent recurrence, resources for patientsâ€”1 year free credit monitoring and identity theft protection ($85K cost), dedicated call center for patient questions. Media notification: Not required (only if >500 individuals in same state/jurisdiction, this was multi-state so under threshold per jurisdiction). Remediation: Process improvementsâ€”automated VPN/system access revocation integrated with HR termination workflow (implemented within 30 days), Epic access certification monthly for high-privilege accounts, quarterly access reviews for all users, employee training on access logging and monitoring. Post-incident: Total cost $425K (notification mailings $127K, credit monitoring $85K, legal counsel $95K, enhanced audit logging $68K, process improvements $50K), OCR audit riskâ€”breach increases scrutiny, but solid response and remediation reduces risk, reputation impactâ€”minimal due to transparent communication and comprehensive patient protection (credit monitoring), process improvements prevent future similar incidents, lessons learnedâ€”termination process critically important, automated provisioning/deprovisioning needed, access monitoring detected incident (but should have prevented it).

### Example 3: Financial Services DDoS Attack
**Prompt:** Execute DDoS incident response for TradingBank (online banking, 2M customers, 450 Gbps volumetric attack, revenue $50K/hour) achieving <1 hour mitigation.

**Expected Output:** Detection: Network monitoring alerted on traffic spike 10:15 AM, 450 Gbps inbound (normal baseline 5 Gbps), firewall CPU at 100%, online banking intermittently unavailable, customer complaints flooding support, MTTD <5 minutes. Classification: P1 Criticalâ€”customer-facing service down, active attack, revenue impact $50K/hour. Team: Network operations lead (incident commander for DDoS), security team (analyze attack and coordinate mitigation), communications (customer status page updates), executive team notified (CEO, CFO aware). Attack analysis: Volumetric DDoSâ€”UDP amplification attack using NTP/DNS reflection, source IPs distributed globally (botnet), targeting online banking domain and mobile app API, no application-layer component (pure network flood). Immediate containment (10:15-10:35 AM): Activated Cloudflare DDoS protection (already in place but not traffic routed through it normally for cost reasons), BGP routing changes to direct all traffic through Cloudflare scrubbing centers, Cloudflare absorbed 98% of attack traffic, online banking restored 10:35 AM (20 minutes downtime). Sustained mitigation: Cloudflare continued scrubbing for 6 hours until attack subsided, rate limiting on API endpoints to protect backend even if attack shifts, geo-blocking for countries with no customers (reduced attack surface). Attack attribution: No ransom demand (not financially motivated DDoS), no activist group claimed responsibility, timing suggests competitor or disgruntled customer, IP reputation analysis shows known DDoS botnet, law enforcement notified (FBI IC3 report filed). Communication: Customer notificationâ€”status page updated every 15 minutes during incident, apologetic email after restoration offering waived fees for affected customers ($12K goodwill gesture), social media monitoring for customer sentiment, media statementâ€”brief acknowledgment, no details on attack vector (avoid giving attackers feedback). Financial impact: Revenue loss $17K (20 minutes Ã— $50K/hour), Cloudflare DDoS mitigation cost $8K (emergency activation premium), customer goodwill gestures $12K, total cost $37K (vs potential $300K+ if 6-hour outage), reputational impact minimal due to rapid mitigation and transparent communication. Post-incident improvements: Architecture changesâ€”route all traffic through Cloudflare always not just during attacks (eliminates BGP change delay), cost analysis shows $15K/month always-on DDoS protection vs $37K one-time incident (ROI positive), Web Application Firewall (WAF) rules enhanced for application-layer DDoS resilience, Incident playbook updatedâ€”DDoS response now automated (BGP changes scripted, can execute in 2 minutes vs 20). Compliance: No regulatory notification required (no data breach, service disruption only), documented incident for SOC 2 audit (demonstrated incident response capability), lessons learnedâ€”preparedness paid off (pre-existing Cloudflare relationship enabled rapid response), architecture improvement prevents future impact, cost of prevention far less than cost of incidents.

---

## Cross-References

- [Security Operations](security-operations.md) - SOC operations and threat detection integration
- [SIEM & Security Monitoring](../Security-Operations/siem-security-monitoring.md) - Detection and alerting for IR
- [Threat Intelligence](threat-intelligence.md) - Threat intel integration in IR workflow
