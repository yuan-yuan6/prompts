---
category: technology
related_templates:
- data-analytics/Data-Science/feature-engineering.md
- data-analytics/Data-Science/exploratory-analysis.md
- ai-ml-applications/MLOps-Deployment/mlops.md
tags:
- machine-learning
- data-preparation
- feature-engineering
- data-labeling
title: ML Data Preparation Strategy
use_cases:
- Preparing tabular, image, text, and time-series data for machine learning with appropriate cleaning, encoding, and transformation strategies
- Implementing feature engineering pipelines creating domain-specific features, handling categorical encoding, and scaling numerical variables
- Managing data labeling workflows with quality assurance, inter-annotator agreement metrics, and active learning prioritization
industries:
- finance
- healthcare
- manufacturing
- retail
- technology
type: framework
difficulty: intermediate
slug: data-preparation
---

# ML Data Preparation Strategy

## Purpose
Prepare production-quality datasets for machine learning covering data cleaning (missing values, outliers, duplicates), feature engineering (encoding, scaling, creation), data labeling (annotation workflows, quality assurance), and augmentation (class balancing, synthetic data) achieving robust model training with no data leakage.

## ðŸš€ Quick Data Preparation Prompt

> Prepare **[DATA_TYPE]** dataset with **[ROWS]** rows and **[FEATURES]** features for **[MODEL_TYPE]** (classification/regression/NLP/vision). Pipeline: (1) **Cleaning**â€”what missing value strategy (drop/impute mean/median/KNN), outlier handling (IQR/z-score/winsorize), duplicate removal? (2) **Feature engineering**â€”what categorical encoding (one-hot <10 cardinality/target encoding high cardinality), numerical scaling (StandardScaler/MinMaxScaler/RobustScaler for outliers), new features (ratios, lags, interactions)? (3) **Splitting**â€”what train/val/test ratio (70/15/15), stratification for imbalanced classes, temporal split for time-series? (4) **Augmentation**â€”what class balancing (SMOTE/undersampling/class weights), data augmentation (rotation/flip for images, back-translation for text)? (5) **Validation**â€”what leakage checks (no future data, no target encoding on test), distribution verification, documentation? Deliver preprocessing pipeline, feature transformations, validation report.

---

## Template

Prepare {DATASET_NAME} with {DATA_SIZE} for {MODEL_TYPE} achieving {QUALITY_TARGET} data quality with {FEATURE_COUNT} engineered features and {SPLIT_RATIO} train/val/test split.

**DATA CLEANING AND QUALITY ASSURANCE**

Handle missing values with strategy matching data characteristics and missingness pattern. Missing completely at random (MCAR) allows simple approaches: drop rows if <5% missing and dataset large (100K+ rows), impute with mean for numerical features (fast, preserves distribution center), impute with mode for categorical (most common value, maintains category distribution). Missing at random (MAR) requires smarter imputation: KNN imputation uses similar rows (k=5 neighbors, effective for clustered data, computationally expensive), iterative imputation (MICE algorithm) models each feature from others (handles complex relationships, risk of overfitting), domain-specific rules (missing income â†’ unemployed category, missing transaction â†’ no activity). Missing not at random (MNAR) indicates systematic patterns: create binary "is_missing" indicator feature (missingness itself predictive), investigate data collection issues (sensor failures, optional fields), consider separate models for missing versus non-missing segments.

Detect and handle outliers preserving valid extreme values. Statistical detection methods: IQR method (values outside Q1-1.5Ã—IQR to Q3+1.5Ã—IQR flagged, 95% of normal distribution retained, robust to non-normal data), z-score method (values >3 standard deviations flagged, assumes normal distribution, sensitive to existing outliers), modified z-score using MAD (median absolute deviation, more robust than standard z-score). Outlier treatment strategies: winsorization caps at percentiles (1st and 99th percentile common, preserves ranking while limiting extremes), log transformation compresses range (effective for right-skewed financial data, requires positive values), robust scaling uses median and IQR (StandardScaler alternatives for outlier-heavy data), remove only if confirmed data errors (not genuine extreme cases, document removal criteria).

Remove duplicates and validate data integrity. Exact duplicates: identify using all columns or subset of key columns (customer_id + timestamp uniquely identifies transactions), keep first or last occurrence based on business logic, log removal counts for audit trail. Near-duplicates require fuzzy matching: text similarity (Levenshtein distance, cosine similarity on embeddings), record linkage for entity resolution (probabilistic matching on name, address, phone), deduplicate within acceptable threshold documenting decisions. Data type validation: enforce schemas (string fields not containing numbers, dates in ISO format, categorical values in allowed set), range validation (age 0-120, percentages 0-100, future dates invalid for historical data), referential integrity (foreign keys exist, hierarchies consistent).

**FEATURE ENGINEERING STRATEGIES**

Encode categorical variables matching cardinality and model type. Low cardinality (<10 unique values): one-hot encoding creates binary columns per category (simple, interpretable, increases dimensionality), works for linear models and tree-based equally, handle unknown categories at inference (ignore or separate "other" column). Medium cardinality (10-100 values): target encoding replaces category with target mean (powerful for tree models, requires regularization to prevent leakage, calculate on training fold only with smoothing toward global mean), label encoding for ordinal categories (small â†’ medium â†’ large preserves order, integer 0/1/2 encoding), frequency encoding (replace with count, captures popularity without target leakage). High cardinality (100+ values): entity embeddings learned during training (neural networks learn dense representations, transfer from larger models), hash encoding limits dimensionality (feature hashing into fixed buckets, some collision acceptable), aggregated features (mean target per category calculated historically, not on current dataset).

Scale numerical features matching model requirements. StandardScaler (z-score normalization) for linear models: transforms to zero mean, unit variance (required for SVM, logistic regression, neural networks, ridge/lasso), fit on training data only (apply same transformation to validation/test preventing leakage), stores mean and std for inference pipeline. MinMaxScaler for bounded algorithms: transforms to [0,1] range (suitable for neural networks with sigmoid activation, image pixels), sensitive to outliers (single extreme value compresses all others), useful when features have known bounds. RobustScaler for outlier-heavy data: uses median and IQR instead of mean/std (outliers don't affect scaling parameters), better for skewed distributions, preserves outlier information without dominating. Tree-based models don't require scaling: XGBoost, Random Forest, LightGBM handle raw values (split-based, invariant to monotonic transformations), scaling may slightly help gradient boosting convergence but not required.

Create domain-specific features capturing business knowledge. Temporal features for time-series: lag features (value 1/7/30 days ago capturing recency effects), rolling statistics (7-day mean, 30-day max capturing trends), time decomposition (hour of day, day of week, month, is_weekend, is_holiday), differences and growth rates (day-over-day change, week-over-week percent change). Interaction features: ratios capturing relationships (revenue per customer, cost per unit, conversion rate), products for combined effects (age Ã— income for purchasing power), polynomial features for nonlinearity (squared terms, cross products for linear models). Aggregation features: customer-level rollups (total purchases, average order value, recency of last orderâ€”RFM), entity statistics (mean rating per product, category-level metrics), window functions (rank within group, percentile position).

**DATA LABELING AND ANNOTATION**

Design labeling workflow balancing quality and cost. In-house labeling for sensitive or complex domains: domain experts label (higher accuracy, expensive, limited scale), build detailed annotation guidelines (10+ page document with examples, edge cases, decision trees), training and calibration sessions (align annotators before production labeling). Outsourced labeling for scale: platforms like Scale AI, Labelbox, Amazon MTurk (cost-effective at volume, quality varies), require extensive guidelines and quality checks, start with small pilot batch (100-500 samples) validating quality before scaling. Programmatic labeling reduces manual effort: labeling functions encoding heuristics (keyword rules, regex patterns, known relationships), Snorkel-style weak supervision (combine multiple noisy labelers probabilistically), distant supervision from knowledge bases (entity linking, relationship extraction from existing data).

Implement quality assurance preventing label noise. Inter-annotator agreement measures consistency: Cohen's Kappa for two annotators (>0.8 substantial agreement, >0.6 moderate, <0.4 poor requiring guideline revision), Fleiss' Kappa for multiple annotators, task-specific metrics (IoU for bounding boxes, BLEU for text). Multi-annotation strategy: double-label 10-20% of data (consensus required for disagreements, identifies ambiguous cases), gold standard set for annotator evaluation (known correct labels, measure individual accuracy), adjudication by expert for disagreements (tie-breaking, guideline clarification). Active learning prioritizes uncertain samples: uncertainty sampling (label samples where model is least confident first, efficient use of annotation budget), query-by-committee (label samples where ensemble models disagree), expected model change (label samples that would most change model parameters).

Handle label noise and maintain label versions. Confident learning identifies mislabels: cleanlab library finds label errors (compares model predictions to given labels, flags likely mistakes), prioritizes high-confidence disagreements for review, iterative cleaning improves dataset quality. Label smoothing regularizes against noise: soft labels (0.9/0.1 instead of 1.0/0.0 for binary), reduces overconfidence on noisy labels, particularly effective for neural networks. Version control labels systematically: semantic versioning for label schema changes (v1.0 â†’ v2.0 for category changes), track label lineage (which annotator, when, which guidelines version), enable rollback and comparison (measure model performance across label versions).

**DATA SPLITTING AND AUGMENTATION**

Split data preventing leakage and ensuring representativeness. Standard random split: 70% train / 15% validation / 15% test (common for medium datasets), 80/10/10 for larger datasets where test sufficient at 10%, stratified split for classification (maintain class proportions in each split, critical for imbalanced classes). Temporal split for time-series: train on historical, validate on next period, test on most recent (simulates production deployment, no future information leakage), expanding window for multiple evaluation periods (train 1-6, val 7, test 8; then train 1-7, val 8, test 9). Group split for clustered data: keep all records of same entity in same split (all transactions from customer X in train or test, not split), prevents data leakage from entity-level patterns, use GroupKFold for cross-validation with groups.

Apply augmentation for imbalanced or small datasets. Class imbalance techniques: SMOTE oversampling (synthetic minority oversampling, creates interpolated samples, effective for tabular data), random undersampling majority class (faster training, may lose information, combine with ensemble methods), class weights in loss function (cost-sensitive learning, no data modification needed, works with any model), focal loss for extreme imbalance (down-weights easy examples, focuses on hard minority cases). Image augmentation: geometric transforms (rotation Â±15Â°, horizontal flip, random crop 90%, scale 0.8-1.2), color augmentation (brightness Â±20%, contrast, saturation, hue shifts), advanced techniques (mixup blending images, cutout masking regions, AutoAugment learned policies). Text augmentation: back-translation (translate to German then back to English, paraphrasing), synonym replacement (WordNet synonyms, EDA easy data augmentation), contextual augmentation (BERT-based word replacement maintaining meaning).

Generate synthetic data when real data insufficient. Tabular synthetic data: CTGAN (conditional tabular GAN, learns joint distribution of features, generates realistic synthetic rows), SMOTE variants (SMOTE-ENN, Borderline-SMOTE for refined synthetic samples), copula-based methods (preserve feature correlations, faster than GANs). Image synthetic data: GANs for realistic images (StyleGAN, Stable Diffusion controlled generation), 3D rendering for labeled data (synthetic scenes with ground truth labels, useful for autonomous driving, robotics), domain randomization (vary textures, lighting, backgrounds for sim-to-real transfer). Privacy-preserving synthetic data: differential privacy guarantees (synthetic data doesn't leak individual records), useful for sharing data externally, validate utility (synthetic should enable similar model performance as real).

**VALIDATION AND DOCUMENTATION**

Detect and prevent data leakage corrupting model evaluation. Target leakage from features: features containing target information (future data, derived from target, proxy variables), detect via suspiciously high feature importance (near-perfect predictors are red flags), careful temporal ordering (no features calculated from future events). Train-test contamination: fit preprocessing only on train (scaler, encoder, imputer fit on train, transform all), no test data in feature selection (cross-validate feature selection within train only), isolated test set until final evaluation (no peeking, no hyperparameter tuning on test). Leakage from data collection: same entity across splits (customer appears in both train and test, model memorizes patterns), data augmentation applied after split (augment only train, never test), temporal leakage (using data from future for training).

Validate data quality before model training. Distribution checks: compare train/validation/test distributions (Kolmogorov-Smirnov test, chi-square for categorical, visual histogram comparison), detect drift from training data (production distribution shift, retrain trigger), verify augmentation preserves distribution (augmented data should match original characteristics). Schema validation: Great Expectations for automated checks (column types, ranges, uniqueness, completeness), data contracts between pipelines (producer guarantees schema, consumer validates), alerting on validation failures (block training on corrupted data).

Document transformations enabling reproducibility. Transformation pipeline as code: scikit-learn Pipeline or ColumnTransformer (reproducible, applies same transforms at inference), feature store for reusable features (Feast, Tecton storing computed features, version controlled), MLflow or DVC tracking data versions (hash of datasets, link to model versions). Data card documentation: dataset description (source, size, time range, sampling), intended use and limitations (what models trained on this, what not suitable for), preprocessing applied (list all transformations with parameters), known issues (quality problems, biases, coverage gaps), ethical considerations (PII handling, potential for harm, fairness implications).

Deliver data preparation as:

1. **CLEANED DATASET** - Missing values handled, outliers treated, duplicates removed, data types validated, quality metrics (completeness >95%, no duplicates, valid ranges)

2. **FEATURE ENGINEERING PIPELINE** - Encoding transformations (categorical strategy per feature), scaling approach (scaler selection rationale), created features (domain features, interactions, temporal), feature selection results

3. **LABELED DATA** - Annotation guidelines document, quality metrics (inter-annotator agreement >0.8), labeling workflow, version-controlled labels, active learning queue

4. **TRAIN/VAL/TEST SPLITS** - Split strategy (random/temporal/group), stratification verification, no leakage validation, size and distribution verification per split

5. **AUGMENTATION PIPELINE** - Class balance strategy (SMOTE parameters, class weights), augmentation transforms (techniques, parameters), synthetic data if used, before/after distribution comparison

6. **DOCUMENTATION** - Data card with provenance, transformation pipeline code, validation report (Great Expectations results), reproducibility instructions, known limitations

---

## Usage Examples

### Example 1: Tabular Classification (Customer Churn)
**Prompt:** Prepare customer churn dataset with 500K rows, 45 features for XGBoost classification achieving <1% missing values and no leakage.

**Expected Output:** Data cleaning: Missing values (3 features >10% missing: last_contact_date 15% â†’ impute with median days, support_tickets 12% â†’ impute with 0 indicating no tickets, income 8% â†’ KNN imputation k=5 using correlated features age and tenure), outliers (monthly_charges: IQR method identifies 2% outliers, winsorize at 1st/99th percentile preserving extreme but valid high spenders, account_age: cap negative values at 0 indicating data error), duplicates (customer_id should be unique, found 234 duplicates, keep most recent record per customer). Feature engineering: Categorical encoding (contract_type 3 values â†’ one-hot encoding, payment_method 4 values â†’ one-hot, product_category 47 values â†’ target encoding with smoothing m=10, calculating mean churn rate per category on training fold only), numerical scaling (not required for XGBoost, but StandardScaler applied for interpretability of SHAP values), temporal features (days_since_last_login, months_as_customer, is_contract_renewal_month binary), domain features (revenue_per_month = total_revenue / tenure_months, support_tickets_per_month, engagement_score combining login frequency + feature usage + support interactions), interaction features (tenure Ã— contract_type indicating loyalty by contract). Splitting: 70/15/15 stratified split (churn rate 18% maintained across splits), group split by household_id (family members kept together preventing leakage), temporal validation (train on 2022-2023, validate Jan-Mar 2024, test Apr-Jun 2024 simulating production). Class imbalance: SMOTE oversampling minority class in training only (18% â†’ 35% churn in augmented train set), class_weight='balanced' as alternative for comparison, focal loss gamma=2 for XGBoost boosting. Validation: Leakage check (removed cancellation_reason featureâ€”only populated after churn event, removed last_bill_date for churnedâ€”contains target information), distribution verification (KS test p>0.05 for all features across splits), feature importance sanity check (no single feature >50% importance indicating leakage). Documentation: Data card documenting 500K customers from CRM export Jan 2022-Jun 2024, preprocessing pipeline in scikit-learn Pipeline saved as joblib, Great Expectations suite validating 45 expectations, known limitation (customers <3 months tenure underrepresented, model may underperform for new customers).

### Example 2: Image Classification (Product Defects)
**Prompt:** Prepare manufacturing defect detection dataset with 50K images, 5 defect classes for CNN achieving balanced classes and robust augmentation.

**Expected Output:** Data collection: 50K images from production line cameras (4K resolution, downsampled to 224Ã—224 for model input), class distribution severely imbalanced (normal 80%, scratch 8%, dent 6%, discoloration 4%, crack 2%), storage in hierarchical folders by class with JSON metadata (timestamp, camera_id, product_batch). Data cleaning: Duplicate detection using perceptual hashing (pHash, 312 near-duplicates removed keeping highest quality), quality filtering (blur detection using Laplacian variance, 856 blurry images removed, exposure filtering removing 234 over/underexposed), corrupted file check (23 unreadable files removed). Image preprocessing: Resize to 224Ã—224 maintaining aspect ratio with padding, normalize to ImageNet statistics (mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225] for transfer learning), color space validation (convert any grayscale to RGB). Splitting: Stratified 70/15/15 split maintaining class proportions, temporal split by production_date (train Jan-Oct, val Nov, test Dec ensuring no future leakage), product_batch grouping (all images from same batch in same split preventing part-level memorization). Class balancing strategy: Training augmentation with class-specific intensity (normal: light augmentation 2x, rare defects: heavy augmentation 10-20x achieving balanced training batches), class weights in loss function as backup (weight inversely proportional to class frequency), focal loss with gamma=2 focusing on hard examples. Augmentation pipeline (Albumentations): Geometric transforms (rotation Â±30Â°, horizontal/vertical flip, affine transforms scale 0.8-1.2, translate Â±10%), color augmentation (brightness/contrast Â±20%, hue/saturation shifts, random shadows), defect-preserving transforms (no transforms that could hide defects: careful with blur, no extreme crops that remove defect regions), test-time augmentation (TTA: 5 augmented versions averaged for inference robustness). Synthetic data generation: Defect overlay technique (real defect patches from labeled regions overlaid on normal products at random positions, physically plausible defect placement), GAN-generated defect images using StyleGAN2 (1K synthetic images per defect class, validated by domain experts, 85% judged realistic), 3D rendering fallback (CAD models with procedural defect generation for crack and dent simulation). Labeling QA: Original labels from quality inspectors (production line decisions), validation labeling by ML team (500 samples double-labeled, 94% agreement, 47 corrections made), active learning queue (model uncertainty sampling flagged 1.2K ambiguous images for expert review, edge cases between scratch and normal refined). Documentation: Dataset card with camera specifications, preprocessing pipeline as PyTorch transforms, augmentation policy saved as Albumentations config YAML, known limitations (lighting variation between cameras, model may require camera-specific calibration).

### Example 3: NLP Text Classification (Support Tickets)
**Prompt:** Prepare customer support ticket dataset with 200K tickets, 12 categories for BERT classification with text cleaning and augmentation.

**Expected Output:** Data collection: 200K support tickets from Zendesk export (ticket_id, subject, body, category, created_date, customer_id), text fields combined (subject + "\n\n" + body for full context), category distribution (billing 25%, technical 20%, shipping 18%, returns 15%, account 10%, other categories <5% each). Text cleaning: HTML removal (BeautifulSoup stripping tags from rich-text tickets), encoding normalization (UTF-8 with unidecode for special characters), PII redaction (regex patterns for email, phone, credit card replaced with [EMAIL], [PHONE], [CARD] tokens), boilerplate removal (signature blocks, auto-replies, forwarded message headers identified and stripped), language filtering (langdetect removing 3% non-English tickets, separate model needed for Spanish 2%). Text preprocessing for BERT: Minimal preprocessing (BERT handles raw text well, preserve punctuation and casing), tokenization using bert-base-uncased tokenizer (WordPiece subword tokenization, max 512 tokens with truncation, 98% of tickets under 256 tokens), special tokens ([CLS] for classification, [SEP] between subject and body). Splitting: Stratified 70/15/15 split by category, customer_id grouping (same customer's tickets in same split preventing customer-specific pattern leakage), temporal consideration (train 2022-2023, val Jan-Jun 2024, test Jul-Dec 2024 for realistic evaluation). Text augmentation: Back-translation (English â†’ German â†’ English, English â†’ French â†’ English creating paraphrases, applied to minority classes only), synonym replacement (WordNet synonyms for 15% of non-stopwords, EDA library), contextual augmentation (BERT-based word replacement maintaining semantic meaning, more sophisticated than random synonyms). Class balancing: Oversampling minority classes with augmentation (target equal representation in training batches), class weights for categories <5% (focal loss alternative), category grouping consideration (merge similar small categories if semantically close: "subscription" + "billing" â†’ "payment_issues"). Labeling quality: Original labels from support agents (assigned at ticket creation), QA audit (1K randomly sampled, 89% agreement with auditor, common confusion between technical/account categories), label correction (312 tickets relabeled after audit, confusion matrix analysis revealing systematic miscategorization), multi-label consideration (15% of tickets arguably belong to multiple categories, primary category used, multi-label model as future improvement). Feature engineering beyond text: Metadata features (ticket_length_words, has_attachment, customer_tenure_days, previous_tickets_count, time_since_last_ticket), customer history (aggregated category distribution of past tickets), temporal features (day_of_week, is_weekend, month). Documentation: Data card with Zendesk export timeframe and filters, preprocessing pipeline as HuggingFace datasets map function, tokenizer saved with model, known limitations (category taxonomy changed in 2023â€”historical tickets relabeled but may have inconsistencies, new product line tickets may need new category).

---

## Cross-References

- [Feature Engineering](../../data-analytics/Data-Science/feature-engineering.md) - Advanced feature creation strategies and selection methods
- [Exploratory Data Analysis](../../data-analytics/Data-Science/exploratory-analysis.md) - EDA techniques informing data preparation decisions
- [MLOps and Model Deployment](../../ai-ml-applications/MLOps-Deployment/mlops.md) - Production data pipelines and feature stores
- [Predictive Modeling Framework](../../data-analytics/predictive-modeling-framework.md) - End-to-end modeling workflow incorporating data preparation
