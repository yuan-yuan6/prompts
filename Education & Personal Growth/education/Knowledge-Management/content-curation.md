---
category: education
related_templates:
- education/curriculum-development.md
- education/curriculum-development-framework.md
tags:
- content-curation
- digital-libraries
- knowledge-repositories
- resource-organization
- readiness-assessment
title: Content Curation & Libraries Readiness Assessment
use_cases:
- Determining readiness to build or improve a curated resource library
- Identifying gaps in collection scope, selection criteria, metadata, discovery UX, and governance
- Producing a practical 30–60 day plan to launch a useful, maintainable collection
industries:
- education
- government
- technology
type: framework
difficulty: intermediate
slug: content-curation
---

# Content Curation & Libraries Readiness Assessment

## Purpose
Assess whether you’re ready to curate and operate a digital library/resource collection by scoring six dimensions: Strategy & Users, Collection Policy, Selection & Acquisition, Metadata & Organization, Discovery & Access, and Operations & Sustainability. Use this as a **go / revise-first** gate before scaling acquisitions or importing large catalogs.

## Template

Conduct a content curation & libraries readiness assessment for {LIBRARY_CONTEXT} curating {COLLECTION_SCOPE} under {PLATFORM_AND_CONSTRAINTS}.

Assess readiness across six dimensions, scoring each 1–5:

**1. STRATEGY & USERS READINESS**
Evaluate whether you have a clear purpose and target users. Confirm primary personas, top use cases (research, onboarding, compliance, learning), and how success will be measured (time-to-find, usage, satisfaction, reuse, cost avoidance).

**2. COLLECTION POLICY READINESS**
Evaluate whether scope is defined and bounded. Confirm what content is in/out, format coverage (articles, datasets, videos), currency expectations, and how you will handle duplicates, outdated items, and “nice-to-have” requests.

**3. SELECTION & ACQUISITION READINESS**
Evaluate how resources enter the collection. Confirm selection criteria (quality, relevance, authority, accessibility), acquisition channels (licensing, purchase, open access, internal uploads), and intake workflow (triage, review, approval).

**4. METADATA & ORGANIZATION READINESS**
Evaluate whether items can be organized and maintained. Confirm a minimal metadata schema (title, description, creator, date, format, rights, tags), classification approach (taxonomy/facets), and guidelines for consistent tagging.

**5. DISCOVERY & ACCESS READINESS**
Evaluate whether users can find and use resources. Confirm search/browse experience, faceted filters, permission/access controls, and accessibility needs. Ensure the discovery UI supports the most common journeys.

**6. OPERATIONS & SUSTAINABILITY READINESS**
Evaluate ownership, maintenance, and lifecycle. Confirm who updates metadata, who weeds/archives content, how feedback is captured, and how usage is monitored. Ensure there’s a plan for ongoing curation, not just launch.

---

## Required Output Format

1. **EXECUTIVE SUMMARY** - Overall readiness score (X.X/5.0), maturity level, go/revise-first, top 3 risks

2. **DIMENSION SCORECARD** - Table: dimension, score (1–5), evidence, biggest gap, highest-impact fix

3. **COLLECTION BLUEPRINT (ONE PAGE)**
- Users + top use cases
- Scope (in/out) + formats
- Selection criteria + intake workflow
- Metadata schema (minimum fields) + taxonomy/facets
- Access policy (who can view/add/edit)

4. **LAUNCH PLAN (30–60 DAYS)**
- Week 1–2: define policy, metadata, taxonomy, and workflow
- Week 3–4: seed content + pilot discovery
- Week 5–8: scale ingestion + measurement loop

5. **RISKS & MITIGATIONS (TOP 5)** - Rights issues, poor metadata, low adoption, staleness, over-scoping

---

## Maturity Scale (1–5)
- **1 — Initial:** Ad-hoc collection; unclear scope; weak metadata; hard to find anything.
- **2 — Developing:** Basic scope and intake exist; inconsistent tagging/rights; limited discovery.
- **3 — Defined:** Clear policy and metadata baseline; usable discovery; ready to pilot/seed.
- **4 — Managed:** Sustainable workflows; measurable usage; routine maintenance and improvements.
- **5 — Optimized:** Evidence-driven curation; high trust; continuous optimization of discovery and value.

---

## Variables (Use Max 3)

| Variable | What to include | Example |
|---|---|---|
| `{LIBRARY_CONTEXT}` | Org/team + setting | “City agency shared resource library for program managers” |
| `{COLLECTION_SCOPE}` | Topics + formats + boundaries | “Policy templates, guides, and datasets for procurement and grants” |
| `{PLATFORM_AND_CONSTRAINTS}` | Platform + constraints | “SharePoint; limited admin time; strict permissions; mobile use” |

---

## Example (Filled)

**Input**
- `{LIBRARY_CONTEXT}`: “University department digital library for course designers.”
- `{COLLECTION_SCOPE}`: “Instructional design exemplars, rubrics, and accessibility checklists.”
- `{PLATFORM_AND_CONSTRAINTS}`: “Notion; contributors are part-time; must support export to PDF.”

**Output (abridged)**
- Executive summary: 3.0/5 (Defined), **go with a pilot**
- Biggest gaps: inconsistent tagging; unclear intake approvals; no weeding plan
- Next actions (30 days): define scope + selection rubric; adopt a minimal metadata schema; create 6 facets (topic, format, level, audience, updated date, rights); seed 50 high-value items; pilot with 10 users; add monthly review and a request form tied to backlog

---

## Best Practices (8)

1. Start with user journeys and “top 20” needs.
2. Keep scope explicit; say “no” to protect quality.
3. Use a selection rubric; avoid “link dumps.”
4. Standardize minimal metadata first; expand later.
5. Make discovery usable: search + browse + facets.
6. Treat rights and accessibility as non-negotiable.
7. Seed with high-quality content before inviting broad contributions.
8. Operate a lifecycle: add, improve, retire—continuously.

---

## Related Resources
- Curriculum sequencing patterns: `curriculum-development.md`
- Rollout framing and iteration: `curriculum-development-framework.md`
