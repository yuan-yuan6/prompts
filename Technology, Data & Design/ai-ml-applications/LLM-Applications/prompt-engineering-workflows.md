---
category: ai-ml-applications
last_updated: 2025-11-22
title: Prompt Engineering & Optimization Workflows
tags:
- prompt-engineering
- prompt-optimization
- prompt-testing
- llm-prompts
use_cases:
- Systematically developing and optimizing prompts for LLM applications
- Testing and evaluating prompt variations for quality improvements
- Creating reusable prompt templates and patterns
- Improving LLM output quality through iterative prompt refinement
related_templates:
- ai-ml-applications/LLM-Applications/llm-application-development.md
- ai-ml-applications/LLM-Applications/rag-systems.md
- ai-ml-applications/AI-Product-Development/ai-product-evaluation.md
industries:
- technology
- finance
- healthcare
- retail
- manufacturing
type: template
difficulty: intermediate
slug: prompt-engineering-workflows
---

# Prompt Engineering & Optimization Workflows Template

## Purpose
Systematically develop, test, and optimize prompts to achieve reliable, high-quality outputs from LLMs through structured engineering processes and evaluation frameworks.

## ðŸš€ Quick Engineering Prompt

> Design an **optimized prompt** for **[TASK]** that achieves **[QUALITY GOAL]** with **[CONSISTENCY REQUIREMENT]**. Guide me through: (1) **Prompt structure**â€”what system prompt pattern, role definition, and output format constraints? Should I use XML tags, chain-of-thought, or few-shot examples? (2) **Edge case handling**â€”how to handle ambiguous inputs, prevent refusals, and ensure graceful degradation? (3) **Optimization workflow**â€”how to A/B test prompts, measure quality (human eval vs. LLM-as-judge), and version control prompts? (4) **Production patterns**â€”how to templatize, handle prompt injection, and maintain consistency across updates? Provide prompt templates, evaluation rubrics, testing methodology, and an optimization checklist.

**Usage:** Replace bracketed placeholders with your specifics. Use as a prompt to an AI assistant for rapid prompt engineering guidance.

---

## Quick Start

**Need to engineer a prompt quickly?** Use this streamlined approach:

### Minimal Example
```python
# Basic prompt engineering pattern
SYSTEM_PROMPT = """You are a [ROLE].

Task: [SPECIFIC_TASK]

Output format: [FORMAT]

Rules:
1. [RULE_1]
2. [RULE_2]
3. [RULE_3]
"""

USER_TEMPLATE = """[CONTEXT]

Input: {user_input}

[SPECIFIC_INSTRUCTIONS]"""

# Test and iterate
test_cases = [
    ("Sample input 1", "Expected output 1"),
    ("Sample input 2", "Expected output 2"),
]

for input_text, expected in test_cases:
    result = llm.generate(SYSTEM_PROMPT, USER_TEMPLATE.format(user_input=input_text))
    print(f"Match: {evaluate(result, expected)}")
```

### When to Use This
- Building new LLM features requiring reliable outputs
- Improving existing prompt performance
- Standardizing prompt patterns across teams
- Debugging inconsistent LLM behaviors
- Optimizing for cost, latency, or quality

### Basic 4-Step Workflow
1. **Define Success** - Specify exact output requirements and evaluation criteria
2. **Draft & Test** - Create initial prompt and test with diverse examples
3. **Iterate & Refine** - Systematically improve based on failure analysis
4. **Validate & Document** - Confirm performance meets requirements, document pattern

---

## Template

```
You are an expert prompt engineer specializing in optimizing LLM applications. Help me develop and optimize a prompt for [TASK_DESCRIPTION] that achieves [SUCCESS_CRITERIA] using [LLM_MODEL] with [CONSTRAINTS].

TASK CONTEXT:
Task Definition:
- Task name: [TASK_NAME]
- Task type: [CLASSIFICATION/GENERATION/EXTRACTION/REASONING/SUMMARIZATION]
- Input format: [INPUT_DESCRIPTION]
- Output format: [OUTPUT_DESCRIPTION]
- Success criteria: [MEASURABLE_CRITERIA]
- Current baseline: [CURRENT_PERFORMANCE]

Use Case:
- Application: [APPLICATION_CONTEXT]
- Users: [WHO_USES_IT]
- Volume: [EXPECTED_USAGE]
- Latency requirement: [MAX_RESPONSE_TIME]
- Cost constraint: [BUDGET_LIMIT]

Quality Requirements:
- Accuracy target: [PERCENTAGE]
- Consistency: [CONSISTENCY_REQUIREMENT]
- Edge case handling: [EDGE_CASES]
- Safety requirements: [SAFETY_CONSTRAINTS]

### 1. TASK SPECIFICATION

Detailed Task Description:
Input: [DETAILED_INPUT_SPECIFICATION]
- Format: [JSON/TEXT/MARKDOWN/OTHER]
- Length range: [MIN_TO_MAX_LENGTH]
- Variability: [INPUT_VARIATIONS]
- Edge cases: [UNUSUAL_INPUTS]

Example Inputs:
1. [EXAMPLE_INPUT_1]
2. [EXAMPLE_INPUT_2]
3. [EXAMPLE_INPUT_3]

Output: [DETAILED_OUTPUT_SPECIFICATION]
- Format: [JSON/TEXT/MARKDOWN/OTHER]
- Required fields: [FIELD_LIST]
- Constraints: [OUTPUT_CONSTRAINTS]
- Examples: [EXAMPLE_OUTPUTS]

Example Input-Output Pairs:
```
Input: "Customer complaint: Charged twice for same order #12345"
Output: {
  "category": "billing",
  "sentiment": "frustrated",
  "priority": "high",
  "action_needed": "investigate_duplicate_charge",
  "order_id": "12345"
}
```

Success Criteria:
- Quantitative: [METRICS_AND_THRESHOLDS]
  - Accuracy: [TARGET_PERCENTAGE]
  - F1 score: [TARGET_SCORE]
  - Latency: [MAX_TIME]
- Qualitative: [SUBJECTIVE_CRITERIA]
  - Tone appropriateness
  - Brand voice alignment
  - User satisfaction

### 2. BASELINE PROMPT DEVELOPMENT

Initial Prompt Structure:
```
SYSTEM PROMPT:
[ROLE_DEFINITION]

[TASK_DESCRIPTION]

[OUTPUT_FORMAT_SPECIFICATION]

[RULES_AND_CONSTRAINTS]

[EXAMPLES_IF_NEEDED]

USER MESSAGE:
[CONTEXT_SECTION]

[INPUT_SECTION]

[SPECIFIC_INSTRUCTIONS]
```

Baseline Prompt V1:
```
System: [YOUR_FIRST_ATTEMPT]

User Template: [USER_MESSAGE_TEMPLATE]
```

Test on Diverse Examples:
```python
# Create diverse test set
test_cases = [
    # Happy path
    {"input": "[NORMAL_CASE]", "expected": "[OUTPUT]"},

    # Edge cases
    {"input": "[EDGE_CASE_1]", "expected": "[OUTPUT]"},
    {"input": "[EDGE_CASE_2]", "expected": "[OUTPUT]"},

    # Error cases
    {"input": "[INVALID_INPUT]", "expected": "[ERROR_HANDLING]"},
]

# Evaluate baseline
for test in test_cases:
    result = llm.complete(prompt.format(**test))
    score = evaluate(result, test["expected"])
    print(f"Test: {test['input'][:50]}... Score: {score}")
```

Initial Results:
- Accuracy: [BASELINE_ACCURACY]
- Common failures: [FAILURE_PATTERNS]
- Areas for improvement: [IMPROVEMENT_OPPORTUNITIES]

### 3. SYSTEMATIC OPTIMIZATION

Optimization Framework:
```
For each iteration:
1. Identify failure pattern
2. Hypothesize root cause
3. Apply targeted improvement
4. Test and measure impact
5. Keep if improvement, revert if degradation
```

Optimization Techniques:

**Technique 1: Clarity & Specificity**
Problem: Ambiguous instructions leading to inconsistent outputs

Before:
```
"Analyze this customer message and categorize it."
```

After:
```
"Classify this customer message into exactly ONE category:
- billing (payment, invoices, charges)
- technical (bugs, errors, performance)
- account (login, password, settings)
- other (anything else)

Output format: {"category": "billing|technical|account|other", "confidence": 0.0-1.0}"
```

Impact: [MEASURE_IMPROVEMENT]

**Technique 2: Output Formatting**
Problem: Inconsistent output format, difficult to parse

Before:
```
"Extract key information from the document."
```

After:
```
"Extract information in this EXACT JSON format:
{
  "company_name": "string or null",
  "amount": number or null,
  "date": "YYYY-MM-DD or null",
  "confidence": 0.0-1.0
}

Rules:
- Use null if information not found
- Dates must be YYYY-MM-DD format
- Amount must be numeric (no currency symbols)
- Never add fields not in the schema"
```

Impact: [MEASURE_IMPROVEMENT]

**Technique 3: Few-Shot Examples**
Problem: Model doesn't understand desired output style

Before:
```
"Rewrite this email professionally."
```

After:
```
"Rewrite this email professionally. Examples:

Input: "hey can u send me the report asap"
Output: "Hello, could you please send me the report at your earliest convenience? Thank you."

Input: "this is wrong fix it"
Output: "I've noticed an issue with this item. Could you please review and make the necessary corrections?"

Now rewrite: {user_email}"
```

Impact: [MEASURE_IMPROVEMENT]

**Technique 4: Chain of Thought**
Problem: Complex reasoning tasks have low accuracy

Before:
```
"Determine if this insurance claim should be approved."
```

After:
```
"Determine if this insurance claim should be approved.

Think through this step-by-step:
1. Check if policy was active on date of incident
2. Verify the incident type is covered under policy
3. Confirm claim amount is within coverage limits
4. Check for any exclusions that apply
5. Make final determination

Show your reasoning for each step, then provide final decision."
```

Impact: [MEASURE_IMPROVEMENT]

**Technique 5: Role & Persona**
Problem: Output tone or expertise level is wrong

Before:
```
"Explain this medical term."
```

After:
```
"You are a patient-facing medical educator who explains complex terms simply.

Explain medical terms as if speaking to a patient with no medical background:
- Use everyday language
- Avoid jargon or define it
- Use analogies when helpful
- Be reassuring and clear

Now explain: {medical_term}"
```

Impact: [MEASURE_IMPROVEMENT]

**Technique 6: Constraint Specification**
Problem: Model violates important constraints

Before:
```
"Generate a product description."
```

After:
```
"Generate a product description following these STRICT constraints:
- Length: 50-100 words exactly
- Tone: Professional but friendly
- Include: key features, benefits, call-to-action
- Exclude: pricing, availability, technical specs
- Never: make unverified claims, use superlatives without basis

Constraints are MANDATORY. Violating them means failure."
```

Impact: [MEASURE_IMPROVEMENT]

### 4. ADVANCED TECHNIQUES

Technique: Structured Prompting
```xml
<prompt>
  <role>Expert financial analyst</role>

  <task>
    <description>Analyze company financial health</description>
    <input_format>Financial statements (10-K excerpts)</input_format>
    <output_format>Structured analysis with risk scores</output_format>
  </task>

  <instructions>
    <step1>Calculate key financial ratios</step1>
    <step2>Identify trends over 3-year period</step2>
    <step3>Flag any concerning patterns</step3>
    <step4>Provide risk score (1-10)</step4>
  </instructions>

  <constraints>
    <constraint>Base analysis only on provided data</constraint>
    <constraint>Do not make predictions or recommendations</constraint>
    <constraint>Cite specific numbers from statements</constraint>
  </constraints>

  <output_schema>
    {
      "ratios": {...},
      "trends": {...},
      "concerns": [...],
      "risk_score": 1-10
    }
  </output_schema>
</prompt>
```

Technique: Meta-Prompting
```
"Before answering the user's question, first:
1. Identify what type of question this is
2. Determine what information you need
3. Plan your response structure
4. Then provide your answer

User question: {user_input}"
```

Technique: Self-Consistency
```python
# Generate multiple outputs, use most common
def self_consistency_completion(prompt, n=5):
    responses = []
    for _ in range(n):
        response = llm.complete(prompt, temperature=0.7)
        responses.append(response)

    # Return most common response
    return mode(responses)
```

Technique: Prompt Chaining
```python
# Break complex task into steps
def multi_step_analysis(document):
    # Step 1: Extract facts
    facts = llm.complete(f"Extract key facts from: {document}")

    # Step 2: Analyze facts
    analysis = llm.complete(f"Analyze these facts: {facts}")

    # Step 3: Generate recommendation
    recommendation = llm.complete(
        f"Based on this analysis: {analysis}, provide recommendation"
    )

    return recommendation
```

### 5. EVALUATION FRAMEWORK

Automated Evaluation:
```python
class PromptEvaluator:
    def __init__(self, test_set):
        self.test_set = test_set

    def evaluate_prompt(self, prompt_template):
        results = {
            "accuracy": 0,
            "latency": [],
            "cost": 0,
            "failures": []
        }

        for test_case in self.test_set:
            start = time.time()

            # Generate response
            response = llm.complete(
                prompt_template.format(**test_case["input"])
            )

            latency = time.time() - start

            # Evaluate accuracy
            is_correct = self.check_accuracy(
                response,
                test_case["expected"]
            )

            results["accuracy"] += int(is_correct)
            results["latency"].append(latency)
            results["cost"] += calculate_cost(prompt_template, response)

            if not is_correct:
                results["failures"].append({
                    "input": test_case["input"],
                    "expected": test_case["expected"],
                    "actual": response
                })

        results["accuracy"] /= len(self.test_set)
        results["avg_latency"] = np.mean(results["latency"])

        return results

    def check_accuracy(self, response, expected):
        """Implement task-specific accuracy check"""
        # For classification
        if "category" in expected:
            return response["category"] == expected["category"]

        # For generation (use LLM-as-judge)
        return self.llm_judge(response, expected)

    def llm_judge(self, response, expected):
        """Use LLM to evaluate quality"""
        judge_prompt = f"""
        Evaluate if this response meets requirements.

        Expected: {expected}
        Actual: {response}

        Does it meet requirements? Answer yes or no.
        """
        result = llm.complete(judge_prompt)
        return "yes" in result.lower()
```

Evaluation Metrics:
- Task accuracy: [ACCURACY_CALCULATION]
- Consistency: [CONSISTENCY_MEASURE]
- Latency: [P50_P95_P99]
- Cost per request: [COST_CALCULATION]
- Failure analysis: [FAILURE_CATEGORIZATION]

### 6. A/B TESTING & EXPERIMENTATION

A/B Testing Framework:
```python
class PromptABTest:
    def __init__(self, prompt_a, prompt_b, traffic_split=0.5):
        self.prompt_a = prompt_a
        self.prompt_b = prompt_b
        self.split = traffic_split
        self.results = {"a": [], "b": []}

    def route_request(self, user_input):
        """Route request to A or B variant"""
        variant = "a" if random.random() < self.split else "b"
        prompt = self.prompt_a if variant == "a" else self.prompt_b

        response = llm.complete(prompt.format(input=user_input))

        self.results[variant].append({
            "input": user_input,
            "response": response,
            "timestamp": time.time()
        })

        return response

    def analyze_results(self):
        """Statistical analysis of A vs B"""
        a_metrics = self.calculate_metrics(self.results["a"])
        b_metrics = self.calculate_metrics(self.results["b"])

        # Statistical significance test
        p_value = self.t_test(a_metrics, b_metrics)

        return {
            "variant_a": a_metrics,
            "variant_b": b_metrics,
            "winner": "a" if a_metrics["score"] > b_metrics["score"] else "b",
            "significant": p_value < 0.05,
            "p_value": p_value
        }
```

Experiment Design:
- Hypothesis: [WHAT_YOU_EXPECT_TO_IMPROVE]
- Variants: [DESCRIBE_DIFFERENCES]
- Sample size: [HOW_MANY_REQUESTS]
- Success metric: [PRIMARY_METRIC]
- Duration: [TEST_DURATION]

### 7. PROMPT VERSIONING & MANAGEMENT

Version Control:
```python
# prompts/customer_support_v1.py
VERSION = "1.0.0"
CREATED = "2025-11-01"
AUTHOR = "team@company.com"

SYSTEM_PROMPT = """..."""
USER_TEMPLATE = """..."""

METRICS = {
    "accuracy": 0.87,
    "avg_latency_ms": 450,
    "cost_per_request": 0.002
}

CHANGELOG = """
v1.0.0 (2025-11-01):
- Initial version
- Baseline accuracy: 87%
"""

# prompts/customer_support_v2.py
VERSION = "2.0.0"
CHANGES = """
- Added few-shot examples (+5% accuracy)
- Structured JSON output (improved parsing)
- Added confidence scores
"""
```

Prompt Registry:
```python
class PromptRegistry:
    """Central registry for all prompts"""

    def __init__(self):
        self.prompts = {}

    def register(self, name, version, prompt, metadata):
        key = f"{name}:{version}"
        self.prompts[key] = {
            "prompt": prompt,
            "metadata": metadata,
            "registered_at": datetime.now()
        }

    def get(self, name, version="latest"):
        if version == "latest":
            version = self._get_latest_version(name)
        return self.prompts[f"{name}:{version}"]

    def rollback(self, name, version):
        """Rollback to previous version"""
        self.prompts[f"{name}:latest"] = self.prompts[f"{name}:{version}"]
```

### 8. DOCUMENTATION & KNOWLEDGE SHARING

Prompt Documentation Template:
````markdown
# Prompt: Customer Support Ticket Classification

## Metadata
- Version: 2.1.0
- Last updated: 2025-11-12
- Owner: @support-ai-team
- Model: Claude Sonnet 4.5

## Purpose
Classify customer support tickets into categories and priority levels
to enable intelligent routing and SLA management.

## Performance
- Accuracy: 94% (target: 90%)
- Avg latency: 380ms (target: <500ms)
- Cost per request: $0.0018
- Test set: 5,000 labeled tickets

## Prompt
```python
SYSTEM_PROMPT = """..."""
USER_TEMPLATE = """..."""
```

## Usage
```python
from prompts import customer_support_v2

result = llm.complete(
    customer_support_v2.SYSTEM_PROMPT,
    customer_support_v2.USER_TEMPLATE.format(ticket=ticket_text)
)
```

## Test Cases
- âœ… Billing inquiries (98% accuracy)
- âœ… Technical issues (93% accuracy)
- âš ï¸  Mixed issues (87% accuracy - known limitation)
- âœ… Non-English (89% accuracy via auto-translation)

## Known Limitations
1. Struggles with tickets containing multiple issues
2. May misclassify new product features not in training
3. Requires English input (or pre-translation)

## Iteration History
- v1.0: Baseline prompt (87% accuracy)
- v1.5: Added few-shot examples (+3%)
- v2.0: Structured output format (+2%)
- v2.1: Improved priority detection (+2%)
````

### 9. PRODUCTION CONSIDERATIONS

Deployment Checklist:
- [ ] Prompt tested on diverse test set (>100 examples)
- [ ] Edge cases handled gracefully
- [ ] Performance meets SLA requirements
- [ ] Cost per request within budget
- [ ] Safety/security review completed
- [ ] Prompt versioned and documented
- [ ] Monitoring and alerts configured
- [ ] Rollback procedure tested

Monitoring in Production:
```python
# Track prompt performance in production
def log_prompt_execution(prompt_version, input, output, metadata):
    metrics = {
        "prompt_version": prompt_version,
        "timestamp": datetime.now(),
        "latency_ms": metadata["latency"],
        "tokens_used": metadata["tokens"],
        "cost": metadata["cost"],
        "user_feedback": None  # Filled later
    }

    # Log to monitoring system
    logger.info("prompt_execution", extra=metrics)

    # Check for degradation
    if metrics["latency_ms"] > LATENCY_THRESHOLD:
        alert("Prompt latency spike", metrics)
```

Continuous Improvement:
- Weekly: Review failure cases
- Monthly: A/B test new variants
- Quarterly: Major prompt refactor based on learnings
```

## Variables

### TASK_DESCRIPTION
The specific task your prompt needs to accomplish.
**Examples:**
- "Classify customer support tickets into 5 categories"
- "Extract structured data from unstructured legal documents"
- "Generate personalized email responses maintaining brand voice"
- "Summarize research papers highlighting key findings and methodology"

### SUCCESS_CRITERIA
Measurable criteria for prompt success.
**Examples:**
- "95% classification accuracy, <500ms latency"
- "100% valid JSON output, zero parsing errors"
- "90% user acceptance rate, professional tone"
- "Key findings accuracy verified by domain experts"

### LLM_MODEL
The LLM you're using.
**Examples:**
- "Claude Sonnet 4.5"
- "GPT-4 Turbo"
- "Gemini 1.5 Pro"
- "Llama 3 70B"

## Usage Examples

### Example 1: Customer Support Ticket Classification

**Context:** SaaS company optimizing support ticket routing

```
Develop and optimize a prompt for classifying customer support tickets into
categories and priority levels for automated routing.

TASK CONTEXT:
- Task name: Support Ticket Classifier
- Task type: CLASSIFICATION
- Input: Customer email/ticket text (50-500 words)
- Output: JSON with category, priority, confidence
- Success criteria: 95% classification accuracy
- Current baseline: 82% accuracy with simple prompt

Use Case:
- Application: Internal support tool
- Users: Customer support team (50 agents)
- Volume: 2,000 tickets/day
- Latency: <2 seconds
- Cost: <$0.01 per classification

Categories:
- billing (invoices, payments, refunds)
- technical (bugs, errors, how-to)
- account (login, settings, permissions)
- feature_request (suggestions, feedback)
- urgent (outages, security, data loss)
```

**Optimization Journey:**
1. **Baseline (82%):** Simple "classify this ticket" prompt
2. **v2 (88%):** Added category definitions with examples
3. **v3 (92%):** Added confidence scoring, uncertainty handling
4. **v4 (95%):** Few-shot examples for edge cases, chain-of-thought for multi-issue tickets

### Example 2: Content Summarization

**Context:** News aggregator app needing article summaries

```
Develop a prompt for summarizing news articles that maintains factual accuracy
while creating engaging, readable summaries.

TASK CONTEXT:
- Task name: News Article Summarizer
- Task type: SUMMARIZATION
- Input: News article (500-3000 words)
- Output: Summary (50-100 words) + key facts (bullet points)
- Success criteria: 90% user acceptance rate, no factual errors
- Current baseline: 75% acceptance, occasional hallucinations

Quality Requirements:
- Accuracy: Zero fabricated facts
- Style: Engaging, journalistic tone
- Length: Strictly 50-100 words
- Structure: Lead sentence + supporting details

Constraints:
- Never add information not in original
- Preserve attribution ("according to...")
- Maintain neutral tone on controversial topics
```

**Optimization Techniques Applied:**
1. **Fact grounding:** "Only include information explicitly stated in the article"
2. **Output structure:** XML format with separate <summary> and <key_facts> sections
3. **Self-verification:** "After writing, verify each claim appears in the source"
4. **Length control:** "Your summary must be exactly 50-100 words. Count them."

### Example 3: Code Review Assistant

**Context:** Developer tools company building automated code review

```
Develop a prompt for reviewing code changes and providing actionable feedback
on bugs, security issues, and best practices.

TASK CONTEXT:
- Task name: Automated Code Reviewer
- Task type: GENERATION (structured feedback)
- Input: Git diff with context
- Output: List of issues with severity, location, explanation, fix suggestion
- Success criteria: 80% of suggestions accepted by developers
- Current baseline: 60% acceptance, too many false positives

Technical Context:
- Languages: Python, JavaScript, TypeScript
- Focus areas: Security, performance, maintainability
- Constraints: No style-only comments (handled by linters)

Output Schema:
{
  "issues": [
    {
      "severity": "critical|high|medium|low",
      "line": number,
      "category": "security|bug|performance|best_practice",
      "issue": "Description",
      "suggestion": "How to fix",
      "confidence": 0.0-1.0
    }
  ],
  "summary": "Overall assessment"
}
```

**Key Optimizations:**
1. **Reduce false positives:** Added confidence scores, threshold at 0.7
2. **Better context:** Include surrounding code, not just diff
3. **Severity calibration:** Explicit definitions with examples
4. **Actionable suggestions:** Require specific fix, not just problem identification

## Best Practices

### Prompt Design
1. **Be specific** - Ambiguity leads to inconsistency
2. **Use examples** - Few-shot examples dramatically improve quality
3. **Structure output** - Request JSON/XML for reliable parsing
4. **Test edge cases** - Include unusual inputs in test set
5. **Iterate systematically** - Change one thing at a time

### Optimization Process
1. **Measure first** - Establish baseline before optimizing
2. **Focus on failures** - Analyze what goes wrong, not what works
3. **Hypothesis-driven** - Have theory for why change will help
4. **Validate improvements** - Measure impact of each change
5. **Document learnings** - Share patterns that work

### Evaluation
1. **Diverse test set** - Cover typical cases, edge cases, adversarial cases
2. **Automated metrics** - Don't rely on manual review alone
3. **Real-world testing** - Shadow production traffic
4. **User feedback** - Track actual user satisfaction
5. **Continuous monitoring** - Watch for degradation over time

### Team Collaboration
1. **Version control** - Track all prompt changes
2. **Centralized registry** - Single source of truth for production prompts
3. **Documentation** - Explain why prompts work, not just what they are
4. **Knowledge sharing** - Regular reviews of best-performing prompts
5. **Reusable patterns** - Build library of proven techniques

## Common Pitfalls

âŒ **Vague instructions** - "Analyze this document and provide insights"
âœ… Instead: Specific task, format, constraints, examples

âŒ **No systematic testing** - Manual spot-checking instead of test suite
âœ… Instead: Automated evaluation on diverse test set

âŒ **Premature optimization** - Tweaking before understanding failures
âœ… Instead: Analyze failure modes, then targeted improvements

âŒ **Ignoring cost** - Using most powerful model for all tasks
âœ… Instead: Match model capability to task complexity

âŒ **No version control** - Losing track of what changed and why
âœ… Instead: Git-based versioning with metadata

âŒ **Overfitting to test set** - Optimizing for specific examples
âœ… Instead: Diverse test set, hold-out validation set

âŒ **Subjective evaluation** - "It looks good to me"
âœ… Instead: Measurable metrics, statistical significance

âŒ **Static prompts** - Set it and forget it
âœ… Instead: Continuous monitoring and improvement

## Related Resources

**Tools:**
- PromptLayer - Prompt versioning and analytics
- PromptFoo - Automated prompt testing
- LangSmith - Prompt debugging and monitoring
- Weights & Biases - Experiment tracking

**Techniques:**
- Chain-of-Thought prompting
- ReAct (Reasoning + Acting)
- Self-consistency
- Constitutional AI

**Documentation:**
- Anthropic Prompt Engineering Guide
- OpenAI Prompt Engineering Guide
- Google Gemini Best Practices

---

**Last Updated:** 2025-11-12
**Category:** AI/ML Applications > LLM Applications
**Difficulty:** Intermediate
**Estimated Time:** 1-2 weeks for systematic optimization process
