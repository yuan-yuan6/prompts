---
category: technology
title: System Architecture Design Strategy
tags:
- software-architecture
- system-design
- distributed-systems
- scalability
use_cases:
- Designing scalable system architectures for high-traffic applications requiring microservices, API gateways, and distributed databases
- Creating enterprise architectures with security, compliance, and multi-region deployment requirements (SOC2, HIPAA, PCI DSS)
- Planning migration strategies from monolithic applications to cloud-native microservices with zero-downtime deployment
related_templates:
- technology/cloud-architecture-framework.md
- technology/site-reliability-engineering.md
- technology/cloud-migration-strategy.md
industries:
- finance
- government
- healthcare
- retail
- technology
type: framework
difficulty: comprehensive
slug: architecture-design
---

# System Architecture Design Strategy

## Purpose
Design comprehensive system architectures balancing scalability, security, performance, and maintainability including component selection, technology stack decisions, deployment strategies, and quality attribute tradeoffs.

## Template

Design architecture for {SYSTEM_TYPE} serving {TARGET_USERS} with {EXPECTED_LOAD} load requirements using {ARCHITECTURE_STYLE} pattern on {CLOUD_PROVIDER} platform targeting {AVAILABILITY}% uptime with {PERFORMANCE_TARGETS} latency.

**ARCHITECTURE PATTERN SELECTION AND SYSTEM DECOMPOSITION**

Select architecture pattern matching organizational maturity, team size, and deployment frequency. Microservices architecture decomposes system into independently deployable services: each service owns single bounded context (user management, product catalog, order processing, payment gateway, inventory, notifications), communicates via REST/gRPC for synchronous operations and message queues (Kafka, RabbitMQ) for asynchronous events, scales independently based on load (payment service scales 10x during Black Friday while catalog remains stable), enables technology diversity (Node.js for real-time features, Java for transaction-heavy services, Go for high-throughput APIs). Organizations shipping 10+ deploys daily with multiple autonomous teams favor microservices accepting operational complexity (distributed tracing, service mesh, eventual consistency challenges).

Modular monolith balances simplicity with structure: single deployable artifact with clear internal boundaries (separate packages/modules per domain), shared database with schema-level separation or separate schemas per module, enforced module dependencies preventing cross-boundary coupling (dependency analysis tools detecting violations), simpler deployment and debugging while preserving migration path to microservices. Teams of 5-15 developers with weekly/biweekly releases often choose modular monolith achieving 80% of microservices benefits with 20% operational overhead.

Event-driven architecture emphasizes asynchronous communication and loose coupling: services publish domain events to message broker (OrderPlaced, PaymentCompleted, InventoryReserved), interested services subscribe and react (order service publishes OrderPlaced, inventory service reserves stock, shipping service creates shipment, analytics service updates dashboards), enables temporal decoupling (services process events at their own pace, subscribers added without publisher changes), implements choreography (no central orchestrator, services coordinate through events) or orchestration (saga coordinator managing multi-step workflows). Financial services and high-volume e-commerce platforms leverage event-driven patterns achieving eventual consistency across distributed systems.

Establish service boundaries using domain-driven design principles: identify bounded contexts through event storming sessions (domain experts and developers mapping business processes to events and aggregates), define ubiquitous language per context (payment context uses "transaction" while accounting context uses "ledger entry" avoiding semantic confusion), align services to team structure following Conway's Law (one team owns 1-3 related services end-to-end), enforce boundaries through API contracts (OpenAPI specifications, Protocol Buffer schemas, AsyncAPI for events). Services communicate through well-defined interfaces preventing tight coupling: synchronous calls use circuit breakers and timeouts (Resilience4j with 50% failure threshold, 5-second timeout preventing cascade failures), asynchronous events include correlation IDs enabling distributed tracing (Jaeger/Zipkin tracking request flow across 10+ services).

**TECHNOLOGY STACK SELECTION AND COMPONENT DESIGN**

Select programming languages matching service characteristics and team expertise. Backend services use type-safe languages for business logic: Java/Kotlin with Spring Boot (enterprise systems requiring mature ecosystem, Spring Data for database abstraction, Spring Security for auth, extensive library support), Node.js/TypeScript (I/O-heavy services, real-time features using WebSockets, serverless functions with fast cold starts, extensive npm ecosystem), Go (high-throughput services requiring minimal memory footprint, network proxies and API gateways, concurrent processing with goroutines, sub-10ms garbage collection pauses), Python (data processing pipelines, ML model serving, scripting and automation, rapid prototyping). Organizations often adopt polyglot approach: payment processing in Java (ACID transactions, mature financial libraries), real-time notifications in Node.js (WebSocket connections, event-driven I/O), log aggregation in Go (high throughput, low memory).

Choose database technology matching data access patterns and consistency requirements. PostgreSQL handles transactional workloads requiring ACID guarantees: normalized schemas for e-commerce orders/payments/inventory, complex joins and transactions, JSONB columns for semi-structured data, horizontal scaling via read replicas (5-10 read replicas distributing query load) and sharding (hash-based sharding by customer_id, range partitioning by date). MongoDB suits document-oriented data with flexible schemas: product catalogs with varying attributes, user profiles with custom fields, horizontal scaling through sharding (hash-based on _id, compound shard keys for query optimization), aggregation pipelines for analytics. Redis provides sub-millisecond access for hot data: session storage (10-minute TTL for shopping cart sessions), caching API responses (5-minute TTL for product details), real-time leaderboards (sorted sets updated on every transaction), rate limiting (sliding window counters).

Implement polyglot persistence selecting optimal database per service: user service uses PostgreSQL (relational user data, roles/permissions), product catalog uses MongoDB (flexible product attributes, rapid schema evolution), session management uses Redis (fast access, automatic expiration), full-text search uses Elasticsearch (inverted indexes, relevance scoring, faceted search), time-series metrics use TimescaleDB or InfluxDB (efficient time-based queries, automatic data retention). Each service owns its database enforcing loose coupling: no cross-service database queries, data access only through service APIs, eventual consistency between services via events.

Design API layer balancing developer experience with performance. RESTful APIs serve resource-oriented operations: standard HTTP verbs (GET, POST, PUT, PATCH, DELETE), resource naming conventions (/api/v1/products, /api/v1/orders/{orderId}), hypermedia links (HATEOAS) for discoverability, pagination for collections (cursor-based for consistency, offset-based for simplicity), OpenAPI 3.1 specifications (Swagger UI for interactive docs, code generation for clients). GraphQL enables flexible client queries: single endpoint returning exactly requested fields, batching multiple resource requests, real-time subscriptions via WebSockets, schema stitching federating multiple services. gRPC optimizes service-to-service communication: Protocol Buffers binary serialization (5-10x smaller than JSON, 3-5x faster parsing), HTTP/2 multiplexing (single connection for multiple requests), bidirectional streaming (real-time data pipelines), strongly-typed contracts (compile-time validation preventing integration errors).

**SCALABILITY AND PERFORMANCE ARCHITECTURE**

Implement horizontal scaling strategies supporting traffic growth without architectural rewrites. Stateless services scale linearly: application servers store no local state (sessions in Redis, uploaded files in S3), Kubernetes HorizontalPodAutoscaler adds pods based on CPU/memory (target 70% utilization, scale from 3 to 50 pods), load balancers distribute requests (round-robin with health checks, consistent hashing for cache affinity), deploy/scale/rollback independent instances without coordination. Achieve 100x scale by adding servers: 3 pods serving 1K requests/second scales to 300 pods serving 100K requests/second maintaining same per-pod performance.

Design caching layers reducing database load and improving latency. CDN caches static assets at edge locations: CloudFront/Cloudflare serving images, CSS, JavaScript from 200+ global POPs (20-100ms latency reduction for users worldwide), cache-control headers defining TTL (1 year for versioned assets, 1 hour for dynamic content), cache invalidation via versioned URLs or purge APIs. Application-level caches reduce backend calls: Redis caching API responses (5-minute TTL for product listings, 1-hour TTL for category metadata, cache warming on deployment), Caffeine in-memory cache for reference data (countries, currencies, rarely-changing configuration, 50MB heap per instance), cache-aside pattern (read-through for hits, write-through for updates ensuring consistency).

Partition databases horizontally distributing data across shards. Hash-based sharding routes requests: customer_id mod 16 determines shard (evenly distributes data, requires resharding when adding shards), each shard runs on dedicated PostgreSQL instance (16 shards supporting 10M customers, 625K per shard), cross-shard queries avoided through denormalization or eventual consistency. Range partitioning suits time-series data: partition orders table by month (2024-01, 2024-02), old partitions archived to cold storage (S3 Glacier), queries filter by date accessing single partition. Vitess or Citus automate sharding complexity providing transparent horizontal scaling.

Optimize database performance through indexing and query tuning. Create indexes matching query patterns: B-tree indexes on foreign keys and frequently filtered columns, partial indexes for subset queries (WHERE status = 'active'), composite indexes matching query order (WHERE customer_id = X AND created_at > Y), covering indexes including all selected columns avoiding table lookups. Analyze slow queries using EXPLAIN plans: identify sequential scans on large tables, missing indexes, inefficient joins, statistics out-of-date requiring ANALYZE. Connection pooling prevents exhaustion: PgBouncer in transaction mode (100-500 connections pooled to 20-50 database connections), HikariCP for Java applications (validating connections on borrow, detecting leaks).

**SECURITY ARCHITECTURE AND COMPLIANCE**

Implement defense-in-depth security across all layers. Network security isolates components: VPC with private subnets for databases and application servers (no public internet access), public subnets for load balancers and bastion hosts, security groups whitelist specific ports (443 for HTTPS, 5432 for PostgreSQL within VPC), Network ACLs provide subnet-level firewalls, AWS PrivateLink/VPC endpoints access AWS services without internet routing. Web Application Firewall (WAF) protects against attacks: AWS WAF/Cloudflare blocking SQL injection, XSS, rate limiting per IP (1000 requests/5 minutes), geographic blocking, bot detection (challenge-response for suspicious traffic).

Secure authentication and authorization throughout system. OAuth 2.0 + OpenID Connect authenticates users: authorization code flow for web/mobile apps, client credentials for service-to-service, JWT access tokens (15-minute expiration, refresh tokens for seamless renewal, RS256 asymmetric signing preventing token forgery), single sign-on via Auth0/Okta/Cognito. Role-Based Access Control (RBAC) authorizes actions: hierarchical roles (admin > manager > user), permissions assigned to roles (users.create, orders.view, reports.admin), checked at API gateway and service level, attribute-based policies for complex rules (OPA evaluating user attributes, resource properties, environmental context).

Encrypt data protecting confidentiality. At-rest encryption uses AES-256: database encryption (PostgreSQL TDE, MongoDB encryption at rest), S3 server-side encryption (SSE-KMS with customer-managed keys), EBS volume encryption for application data, key rotation every 90 days via AWS KMS or HashiCorp Vault. In-transit encryption mandates TLS 1.3: HTTPS for all external APIs (terminated at load balancer or API gateway), mTLS for service-to-service communication (Istio service mesh issuing certificates, 24-hour certificate lifetimes preventing long-term compromise), certificate management via AWS ACM or cert-manager.

Implement compliance controls meeting regulatory requirements. GDPR compliance requires: data inventory mapping personal data, consent management (explicit opt-in for marketing, granular preferences), right to erasure (user deletion workflows, backup deletion within 30 days), data portability (export user data in JSON format), breach notification procedures (detect within 24 hours, notify within 72 hours). PCI DSS Level 1 mandates: cardholder data encryption, tokenization via payment gateway (Stripe/Braintree eliminating PCI scope), quarterly vulnerability scans, annual penetration testing, security awareness training, change management procedures. HIPAA technical safeguards include: unique user identification, emergency access procedures, automatic logoff after 15 minutes, encryption and decryption, audit controls logging all PHI access.

**DEPLOYMENT AND OPERATIONAL ARCHITECTURE**

Design CI/CD pipelines enabling rapid, safe deployments. Build stage compiles code: GitHub Actions/GitLab CI triggering on PR/merge, Docker multi-stage builds (compile in builder image, copy artifacts to minimal runtime image reducing size 10x), parallel builds (unit tests, integration tests, security scans running concurrently reducing 20-minute pipeline to 6 minutes). Test stage validates quality: unit tests requiring 80% coverage, integration tests with Testcontainers (ephemeral PostgreSQL/Redis instances per test run), contract tests (Pact validating service API compatibility), security scanning (Snyk/Trivy detecting vulnerabilities, blocking critical/high findings).

Implement progressive delivery minimizing deployment risk. Blue-green deployment maintains two production environments: blue environment serves production traffic, green environment receives new deployment, smoke tests validate green, DNS/load balancer switches traffic to green (instant cutover), blue remains available for instant rollback. Canary deployment gradually shifts traffic: deploy new version to 5% of pods, monitor error rate/latency for 10 minutes (automatic rollback if error rate >0.1% or p95 latency >200ms), increment to 25%/50%/100% with validation at each stage, Flagger/Argo Rollouts automating promotion based on Prometheus metrics. Feature flags decouple deployment from release: deploy code with features disabled, enable for internal testing, gradual rollout to 10%/50%/100% users, kill switch for instant disable without redeployment (LaunchDarkly/Split providing targeting rules, gradual rollouts, audit logs).

Design observability stack providing visibility into distributed system. Logging captures request flow: structured JSON logs (timestamp, severity, correlation_id, service, message), ELK Stack (Elasticsearch for storage/search, Logstash for ingestion, Kibana for visualization) or Loki (lower cost, Grafana-native), 30-day retention in hot storage (searchable), 1-year archive in S3. Metrics track system health: Prometheus scraping service endpoints (/metrics exposing RED metrics: Rate of requests, Errors, Duration), custom business metrics (orders per minute, revenue tracking, conversion rates), Grafana dashboards per service and executive summaries, AlertManager routing alerts (PagerDuty for P1/P2, Slack for warnings). Distributed tracing connects request spans: OpenTelemetry instrumentation (automatic for common frameworks, manual for custom code), Jaeger backend (trace storage, dependency graphs, latency analysis), 100% sampling in staging, 10% in production (head-based sampling capturing all traces, tail-based sampling keeping only slow/error traces).

Monitor and enforce SLAs defining acceptable service quality. Define SLIs measuring user experience: availability (successful requests / total requests), latency (p50/p95/p99 response time), throughput (requests per second). Set SLO targets: 99.9% availability (43.8 minutes downtime per month), p95 latency <200ms, establish error budget (0.1% error rate allows 43K failed requests per month at 500M total). Alert on SLO violations: error budget burn rate (consuming 10% of monthly budget in 1 hour triggers page, 2% per hour warns), latency budget exceeded, availability dipping below target. Generate SLO compliance reports monthly showing service reliability trends, error budget consumption, incidents contributing to downtime.

Deliver system architecture as:

1. **ARCHITECTURE DIAGRAMS** - C4 model (Context, Container, Component, Code), data flow diagrams, deployment architecture, network topology

2. **TECHNOLOGY DECISIONS** - Selected technologies with tradeoff analysis (build vs buy, SQL vs NoSQL, monolith vs microservices), cost projections, learning curve assessment

3. **API SPECIFICATIONS** - OpenAPI/GraphQL schemas, Protocol Buffer definitions, AsyncAPI event schemas, versioning strategy, deprecation policy

4. **DEPLOYMENT STRATEGY** - CI/CD pipeline configuration, environment setup (dev/staging/production), blue-green/canary procedures, rollback playbooks, secrets management

5. **SCALING AND RESILIENCE PLAN** - Auto-scaling policies, capacity planning, circuit breaker configuration, disaster recovery procedures (RTO/RPO targets), chaos engineering experiments

6. **SECURITY AND COMPLIANCE** - Threat model, authentication/authorization design, encryption implementation, audit logging, compliance control mapping (GDPR/HIPAA/PCI DSS)

---

## Usage Examples

### Example 1: E-commerce Platform (Microservices)
**Prompt:** Design architecture for ShopFlow e-commerce platform handling 1M daily active users, 10K concurrent users, 500K daily orders requiring 99.99% availability with <200ms p95 API latency.

**Expected Output:** Architecture pattern: Microservices with event-driven choreography, 12 services (user-service, auth-service, product-catalog, search, cart, order, payment, inventory, shipping, notification, recommendation, analytics), each owned by dedicated team, independently deployable via Kubernetes, communicating via synchronous REST/gRPC for queries and asynchronous Kafka events for commands. Technology stack: Frontend React 18 SPA with Next.js 14 SSR for SEO-critical pages (product detail, category), mobile apps (React Native iOS/Android), backend services Node.js/TypeScript (cart, notification real-time features), Java Spring Boot (order, payment transactional consistency), Go (product catalog high throughput), Python (recommendation ML models). Data layer: PostgreSQL for transactional data (user profiles, orders, payments with ACID guarantees, 5 read replicas per service database), MongoDB for flexible schemas (product catalog 10M SKUs with varying attributes, user-generated content), Elasticsearch for full-text search (product search with fuzzy matching, faceted navigation, relevance scoring), Redis for caching (session storage 10M active sessions, product detail cache 5-minute TTL, flash sale inventory hot keys). Infrastructure: AWS deployment, Application Load Balancer with WAF (rate limiting 1000 req/min per IP, SQL injection/XSS blocking), API Gateway (Kong) with rate limiting per API key (10K req/day free tier, 100K paid tier, 429 responses with Retry-After), Kubernetes on EKS (30 nodes c6i.2xlarge, HorizontalPodAutoscaler per service CPU >70% triggers scale-out, 3-50 pods per service), Istio service mesh (mTLS between services, traffic splitting for canary deploys, circuit breakers with 50% failure threshold). Scalability: Horizontal pod autoscaling achieving 100x scale (Black Friday traffic 100K concurrent users, payment service scales 3→50 pods, product catalog 5→30 pods maintaining <200ms latency), database read replicas (5 replicas per service, PgBouncer connection pooling 500→50 connections), caching layers (CloudFront CDN edge caching static assets 200+ global POPs, Redis Cluster 6 nodes with Sentinel failover 100K ops/sec), asynchronous processing (Kafka topics for order processing, inventory updates, shipment creation, 3 consumer groups per topic for parallelism). Security: OAuth 2.0/OIDC authentication (Auth0 integration, JWT access tokens 15-minute expiration, refresh tokens 30-day), RBAC authorization (customer/vendor/admin roles, OPA policies for complex rules), encryption at rest (RDS/S3/EBS AES-256 with KMS, customer-managed keys 90-day rotation), in-transit TLS 1.3 (ALB HTTPS termination, mTLS service-to-service via Istio), PCI DSS Level 1 compliance (payment data tokenized via Stripe, no cardholder data stored, quarterly ASV scans, annual penetration testing). Observability: Prometheus metrics (RED metrics per service, custom business metrics orders/revenue/conversion, 15-second scrape interval), Grafana dashboards (service health, executive summary showing GMV/orders/latency, on-call dashboard for incidents), Jaeger distributed tracing (OpenTelemetry instrumentation, 10% sampling in production, trace-to-log correlation via correlation ID), ELK logging (structured JSON, 30-day retention, 1-year S3 archive). Deployment: GitOps with ArgoCD (GitHub as source of truth, Helm charts per service, automatic sync every 3 minutes), CI/CD via GitHub Actions (build Docker images, run tests 80% coverage required, Trivy security scan, deploy to staging on merge to develop, production deploy via manual approval), canary deployment (Flagger deploying to 5% pods, monitoring error rate/latency 10 minutes, automatic rollback if error rate >0.1%, gradual promotion to 100%), feature flags (LaunchDarkly gradual rollouts, kill switches, user targeting by segment). Costs: $85K/month infrastructure (EKS $300/cluster + EC2 $45K/month for 30 nodes, RDS $18K/month for 12 service databases with replicas, ElastiCache $8K, Elasticsearch $6K, data transfer $3K, monitoring $5K), $15K/month SaaS (Auth0 $2K, Stripe 2.9% + $0.30 per transaction amortized, LaunchDarkly $3K, PagerDuty $2K, DataDog alternative to Prometheus $8K).

### Example 2: Healthcare SaaS (HIPAA-Compliant)
**Prompt:** Design architecture for MedConnect patient management system serving 500 healthcare providers, 100K patients, requiring HIPAA compliance, 99.95% availability with encrypted PHI storage and comprehensive audit logging.

**Expected Output:** Architecture pattern: Modular monolith transitioning to microservices, initial deployment single Spring Boot application with clear module boundaries (patient-module, appointment-module, billing-module, ehr-module, reporting-module), shared PostgreSQL database with schema-per-module isolation, roadmap extracting billing and EHR to separate services within 12 months. Technology stack: Backend Java 17 Spring Boot (mature security libraries, HIPAA-compliant audit frameworks, extensive healthcare integration ecosystem HL7/FHIR), frontend Angular 17 with Angular Material (enterprise UI components, accessibility WCAG AA compliance, strong typing preventing PHI leaks), mobile apps iOS/Android native (biometric authentication, offline PHI access with encrypted local storage, MDM integration for BYOD security). Data layer: PostgreSQL 15 for all structured data (patient records, appointments, billing, 3NF normalization, row-level security policies restricting PHI access by provider organization), encrypted at rest with AWS RDS encryption (AES-256, customer-managed KMS keys with CloudHSM backing for HIPAA compliance, automatic key rotation every 90 days), encrypted backups (automated daily snapshots with 30-day retention, cross-region replication to us-west-2 DR site, point-in-time recovery 5-minute granularity). Infrastructure: Azure deployment (healthcare-specific compliance certifications BAA signed, HITRUST CSF certified, FedRAMP moderate authorized), Virtual Network with NSGs (database in private subnet no internet access, application tier in protected subnet accessed via Application Gateway, bastion host for admin access with JIT enabling), Application Gateway with WAF (OWASP Core Rule Set, rate limiting 500 req/min per user, IP allow-listing for integration partners), AKS cluster (6 nodes Standard_D8s_v3, pod security policies enforcing non-root containers, network policies isolating sensitive workloads). Security: Multi-factor authentication mandatory (Azure AD integration, TOTP via Authenticator app, SMS backup, hardware token support for physicians), RBAC with ABAC policies (roles: patient/nurse/physician/admin, attributes: organization/department/specialty, break-glass emergency access with audit alert), field-level encryption for sensitive PHI (SSN, diagnosis codes encrypted with application-level keys separate from database encryption, Java Cryptography Extension JCE), TLS 1.3 for all connections (mTLS for HL7 integrations, certificate pinning in mobile apps). Compliance: HIPAA technical safeguards (unique user ID for all access, automatic logoff 15 minutes, emergency access procedures documented, encryption/decryption enabled, audit controls comprehensive), audit logging (all PHI access logged with user/IP/timestamp/data accessed, CloudTrail + application logs to SIEM Splunk, 7-year retention for HIPAA, tamper-evident log integrity via blockchain anchoring), BAA with all subprocessors (Azure, SendGrid email, Twilio SMS, Auth0 identity), annual HIPAA risk assessment, quarterly vulnerability scans. Integration: HL7 v2.x interfaces (ADT messages for patient registration, ORM for lab orders, ORU for results, Mirth Connect integration engine), FHIR R4 APIs (RESTful resources Patient/Encounter/Observation, SMART on FHIR app launch for third-party integrations, OAuth 2.0 scopes for granular consent), claims clearinghouse (837 EDI for billing, 835 for remittance, SFTP batch transfers nightly). Observability: Application Insights monitoring (request/dependency/exception telemetry, live metrics stream, application map showing dependencies), custom health checks (database connectivity, HL7 interface status, FHIR endpoint availability, executed every 30 seconds), alerting (PagerDuty for P1/P2: database down, PHI breach detected, audit log pipeline failure, email for lower priority), compliance dashboards (Grafana showing PHI access patterns, failed login attempts, encryption status, data residency verification). Deployment: Blue-green deployment with database compatibility (schema changes backward-compatible for 2 versions, deploy new version to green slots, smoke tests verify FHIR/HL7 integration, traffic switch via Azure Traffic Manager, blue retained 24 hours for rollback), CI/CD via Azure DevOps (build pipeline: Maven build, JUnit tests 85% coverage, OWASP dependency check, SonarQube quality gate, Docker build, Trivy scan, release pipeline: deploy to dev automatically, staging requires QA sign-off, production requires change advisory board approval), database migrations (Flyway versioned migrations, tested in staging 1 week before production, rollback scripts prepared for all schema changes). Disaster recovery: RTO 4 hours, RPO 1 hour (cross-region async replication, automated failover testing quarterly, runbooks for DR activation, failback procedures documented), backup testing monthly (restore random patient subset to verify data integrity, test encryption key recovery). Costs: $32K/month infrastructure (AKS $8K for nodes + $400 cluster management, PostgreSQL $12K with HA and replicas, Application Gateway $2K, VPN Gateway $1.5K, monitoring $3K, backup storage $2K, data egress $3K), $8K/month SaaS (Azure AD Premium $2K for 500 providers, Auth0 healthcare plan $3K with BAA, Splunk SIEM $3K), compliance $15K/month amortized (annual HIPAA audit $60K, penetration testing $40K, security awareness training $20K, cyber insurance $60K).

### Example 3: Fintech Trading Platform (Low-Latency)
**Prompt:** Design architecture for TradeX stock trading platform supporting 50K concurrent traders, 100K orders per second, requiring <10ms order matching latency, 99.999% availability with real-time market data streaming.

**Expected Output:** Architecture pattern: Hybrid microservices with event sourcing, core trading engine single highly-optimized service (written in C++ or Rust for minimal latency, lock-free data structures, LMAX Disruptor pattern for 6M messages/second, runs on dedicated bare-metal servers avoiding noisy neighbor problem), supporting services in microservices (user-service, portfolio-service, risk-management, market-data, notification-service, analytics using Java/Go). Technology stack: Trading engine C++ 20 (mechanical sympathy optimizations, CPU cache-aware data structures, zero-copy message passing, DPDK for kernel-bypass networking achieving <1μs network latency), market data handlers Rust (memory safety without GC pauses, fearless concurrency, WebSocket connections to exchanges NYSE/NASDAQ/CME, 10K connections multiplexed), user-facing services Java Spring Boot (Spring WebFlux reactive streams, non-blocking I/O), frontend React with Redux (real-time order updates via WebSocket, optimistic UI updates, millisecond timestamp precision). Data layer: Event sourcing for all trades (EventStoreDB capturing OrderPlaced/OrderMatched/OrderCancelled events, complete audit trail for regulatory compliance, replay capability for dispute resolution), CQRS separating reads/writes (write side optimized for order entry <10ms, read side materialized views in PostgreSQL for portfolio/positions/history, 100ms staleness acceptable for queries, eventual consistency), TimescaleDB for market data (tick-by-tick prices 100M rows/day, continuous aggregates for OHLCV candles, retention policies archiving >90 days to S3), Redis for real-time state (order book snapshots, user positions, risk limits, sub-millisecond access). Infrastructure: Co-located servers in Equinix financial data centers (NY4, LD4, TY3 proximity to exchanges reducing latency 20ms→2ms), bare-metal Dell PowerEdge R750 (Intel Xeon Platinum 8380 40-core, 512GB RAM, NVMe SSD RAID 10, 100Gbps network, Solarflare NICs with kernel bypass), Kubernetes on bare-metal for non-latency-critical services (user-facing APIs, analytics, reporting tolerate 50-100ms latency), multi-region deployment (primary NY4 serving Americas, DR in LD4 serving Europe, TY3 serving Asia-Pacific, async replication for user data, active-active for market data). Performance optimization: CPU pinning and NUMA awareness (trading engine pinned to cores 0-19 with local memory, OS processes isolated to cores 20-23, interrupts routed to dedicated cores), huge pages reducing TLB misses (1GB huge pages for trading engine memory pool, transparent huge pages disabled for predictable latency), kernel tuning (disable CPU frequency scaling, isolate CPUs from scheduler, increase network buffer sizes, TCP_NODELAY preventing Nagle's algorithm 40ms delay), DPDK kernel bypass (user-space network driver polling NIC, zero-copy DMA, batching for efficiency, achieving 50M packets/second). Security: Financial-grade security (FIX protocol with encryption FIX/FAST, market data authenticated, order validation preventing spoofing/layering manipulation detected via ML models), two-factor authentication mandatory (hardware security keys YubiKey preferred, TOTP backup, biometric for mobile apps), transaction signing (orders signed with user private keys, server validates signature, non-repudiation for regulatory compliance), DDoS protection (dedicated DDoS scrubbing with 100Gbps capacity, rate limiting per user 1000 orders/second, WebSocket connection limits 10 per user). Compliance: SEC Rule 15c3-5 Market Access Rule (pre-trade risk checks <1ms: credit limit, position limit, order size, duplicate order detection, erroneous order prevention), consolidated audit trail CAT (report all orders/trades to FINRA within 24 hours, SIP consolidated tape integration, clock synchronization via NTP stratum-1 within 100ms of UTC), Regulation SCI (system capacity testing quarterly at 2x peak, disaster recovery RTO 2 hours, incident notification to SEC within 24 hours, annual DR testing with regulators). Market data: Direct exchange feeds (NYSE Pillar, NASDAQ TotalView, CME Globex, FIX/FAST protocols, multicast UDP for quotes, TCP for trades, normalized to internal format), market data platform (KDB+ time-series database storing 100M ticks/day, columnar compression reducing storage 10x, kdb+ queries analyzing 1TB data in seconds), WebSocket distribution (server-sent events pushing updates to 50K connected clients, delta compression reducing bandwidth 80%, throttling updates to 100ms for visual rendering limits). Observability: Microsecond-precision logging (hardware timestamp counter TSC for <50ns overhead logging, structured binary logs avoiding serialization, stored in SSD ring buffer, critical path <10 lines logged), metrics (StatsD/Prometheus for business metrics, Telegraf for system metrics CPU/memory/network/disk, p50/p95/p99/p999/max latency histograms, 1-second granularity), distributed tracing (custom lightweight tracing: correlation ID, timestamps at entry/exit of each component, <100ns overhead, aggregated for flame graphs identifying bottlenecks). Deployment: Blue-green deployment with connection draining (deploy new trading engine version to standby servers, smoke test with synthetic orders, DNS cutover to new, old servers drained over 5 minutes allowing in-flight orders to complete), database migration zero-downtime (event store append-only requires no migration, CQRS read models rebuilt in background from events, cutover when caught up), canary testing (1% of non-critical users routed to new version for 1 hour monitoring error rates, automatic rollback if errors >0.01%). Disaster recovery: RTO 2 hours, RPO 1 second (sync replication of orders to DR site, async replication of market data, automated failover via DNS, quarterly DR drills with SEC observation, runbooks for 20+ failure scenarios), backup strategy (event store backed up continuously to S3 with cross-region replication, point-in-time recovery to any second within 7 years for regulatory inquiries). Costs: $180K/month infrastructure (co-location $25K for 10 racks in NY4, bare-metal servers $80K lease for 40 servers, 100Gbps cross-connects to exchanges $30K, Kubernetes cluster $15K, data egress $20K, monitoring $10K), market data fees $120K/month (exchange data fees, SIP feeds, reference data), compliance $50K/month amortized (CAT reporting fees, SEC registration, legal/compliance staff, annual audits).

---

## Cross-References

- [Cloud Architecture Framework](cloud-architecture-framework.md) - Cloud-native patterns, multi-region deployment, serverless architectures
- [Site Reliability Engineering](site-reliability-engineering.md) - SLO/SLA definition, error budgets, incident response, chaos engineering
- [Testing and QA Strategy](testing-qa.md) - Testing pyramid for microservices, contract testing, chaos testing strategies
- [Version Control and Git Workflow](version-control.md) - Trunk-based development supporting continuous deployment, GitOps patterns
