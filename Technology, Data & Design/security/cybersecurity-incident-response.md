---
title: Cybersecurity Incident Response & Threat Management Framework
category: security
tags:
- security
- incident-response
- threat-detection
- forensics
use_cases:
- Responding to active security incidents (ransomware, data breach, DDoS) with detection in <15 min, containment in <1 hour, recovery in <4 hours using NIST 800-61 framework
- Building 24/7 SOC capabilities with SIEM/EDR/NDR integration, threat intelligence feeds, automated playbooks, MTTD <15 min, false positive <10%
- Conducting forensic investigations with disk/memory/network analysis, evidence preservation, chain of custody, legal compliance for incident attribution and prosecution
- Implementing crisis communication plans for executives, customers, regulators with severity-based templates, escalation matrices, notification timelines (GDPR 72 hours, state laws)
related_templates:
- security/Cybersecurity/incident-response.md
- security/Security-Operations/siem-security-monitoring.md
industries:
- finance
- government
- healthcare
- technology
type: framework
difficulty: intermediate
slug: cybersecurity-incident-response
---

# Cybersecurity Incident Response & Threat Management Framework

## Purpose
Comprehensive framework for cybersecurity incident response including threat detection, incident containment, forensic analysis, recovery procedures, threat hunting, security orchestration, and organizational resilience based on NIST 800-61 for protecting digital assets and infrastructure.

## Template

Implement incident response for {ORGANIZATION_NAME} protecting ${ASSET_VALUE} in digital assets, {USER_COUNT} users, {SYSTEM_COUNT} systems, achieving {MTTD_MIN} minute MTTD, {MTTC_HOURS} hour MTTR, {DETECTION_RATE}% threat detection, {RECOVERY_RTO} RTO, and {SECURITY_MATURITY} maturity level.

**NIST 800-61 INCIDENT RESPONSE LIFECYCLE**

Preparation—SOC Setup: 24/7 coverage (in-house analysts for tier 1/2 + managed SOC for nights/weekends, or fully managed via MSSP), technology stack (SIEM for log aggregation/correlation Splunk/Sentinel/Sumo Logic, EDR for endpoint visibility CrowdStrike/SentinelOne/Defender, NDR for network traffic analysis Darktrace/ExtraHop/Vectra, SOAR for automation Palo Alto XSOAR/Splunk Phantom), threat intelligence (commercial feeds CrowdStrike/Recorded Future, OSINT VirusTotal/AbuseIPDB, government sharing CISA/FBI/MS-ISAC, industry ISACs FS-ISAC/H-ISAC/ICS-CERT). Team Structure: Tier 1 (alert triage, false positive filtering, escalation), Tier 2 (investigation, containment, playbook execution), Tier 3/IR Lead (forensics, complex incidents, external coordination), CISO (critical incidents, executive comms, legal liaison). Playbooks: Ransomware (isolate host, verify backups, involve legal/FBI, no ransom policy vs ransom decision matrix), data breach (scope determination, evidence preservation, notification obligations GDPR 72 hours/state laws, credit monitoring), phishing (takedown request, credential reset, user retraining), DDoS (traffic scrubbing, CDN failover, ISP coordination), insider threat (HR involvement, covert monitoring, legal holds). Training: Quarterly tabletop exercises (scenario-based: ransomware encrypts critical systems, customer data exfiltrated, CEO BEC attack), annual red team assessment, monthly security awareness (phishing simulations), new analyst onboarding (60-day ramp with shadowing).

Detection/Analysis—SIEM Correlation Rules: 400+ detection rules mapped to MITRE ATT&CK (initial access T1078 valid accounts, persistence T1053 scheduled tasks, privilege escalation T1068 exploits, defense evasion T1070 indicator removal, credential access T1003 credential dumping, discovery T1087 account discovery, lateral movement T1021 remote services, collection T1560 archive collected data, exfiltration T1041 C2 channel, impact T1486 data encrypted for impact). Use Cases: Failed login anomalies (10+ failures same account 5 min, geographically impossible travel, privileged account auth from non-admin workstation), malware indicators (known bad hashes from threat intel, suspicious PowerShell execution, unsigned drivers loaded, registry persistence keys), data exfiltration (large outbound transfers, connections to known C2 IPs, DNS tunneling patterns), lateral movement (Pass-the-Hash attempts, SMB/RDP from unexpected sources, service account privilege escalation). EDR Detection: Process injection (CreateRemoteThread API calls), fileless malware (memory-only execution, WMI/PowerShell abuse), persistence mechanisms (startup folders, scheduled tasks, registry run keys), behavioral analytics (ransomware behavior crypto API calls + file renaming). Alert Triage: Severity classification (P1 Critical: active ransomware/data exfiltration, immediate response, P2 High: confirmed compromise but contained, 1-hour SLA, P3 Medium: suspicious activity under investigation, 4-hour SLA, P4 Low: policy violation or informational, next business day), false positive handling (tune rules to <10% FP rate, whitelist known-good patterns, feedback loop to detection engineering), escalation criteria (P1/P2 always escalate to tier 2, tier 2 escalates to IR lead if >5 systems affected or privileged account compromise). Forensic Collection: Live response (memory dump with WinPmem/LiME before shutdown, running processes/network connections, volatile artifacts), disk imaging (write-blocker or FTK Imager, verify hash SHA-256, maintain chain of custody), network capture (PCAP from SPAN/TAP, NetFlow analysis, DNS query logs), cloud forensics (AWS CloudTrail, Azure Activity Logs, GCP Audit Logs, snapshot EBS volumes before analysis).

Containment—Short-Term Containment: Network isolation (EDR host isolation CrowdStrike Falcon/SentinelOne, VLAN quarantine via NAC, firewall ACL deny all traffic except management), account lockout (disable in AD/Okta, revoke all active sessions, reset passwords force logout, MFA re-enrollment), malware containment (quarantine infected files, block C2 domains/IPs at firewall/proxy, USB/removable media restrictions). Long-Term Containment: Segmentation (separate compromised systems into isolated VLAN, maintain connectivity for investigation but prevent spread), backup protection (air-gap backup servers, verify backup integrity before recovery, test restore on isolated system), evidence preservation (no reboots until memory captured, maintain disk images, preserve logs export from SIEM before retention expires, document all actions timestamped). Containment Decision Matrix: Ransomware—immediate isolation (single host: EDR isolation, multiple hosts: VLAN quarantine, domain-wide: disconnect internet/VPN gateways, engage external IR firm within 2 hours), Data Breach—preserve evidence priority (no system reboots, enable verbose logging on affected systems, legal hold on all data, coordinate with legal counsel before remediation), Insider Threat—covert monitoring (enable enhanced logging without alerting user, coordinate with HR for suspension decision, involve legal for potential law enforcement), APT/Nation-State—preserve for attribution (minimal disruption to allow threat actor tracking, coordinate with FBI/CISA, extended monitoring before eradication).

Eradication—Threat Removal: Malware eradication (EDR automated remediation for known threats, manual removal for unknown verified with VirusTotal/sandbox analysis, registry key cleanup, scheduled task removal), backdoor elimination (hunt for persistence mechanisms registry run keys/scheduled tasks/services, web shells in IIS/Apache, SSH authorized_keys, golden ticket detection Mimikatz indicators), vulnerability patching (emergency patching if exploited CVE, test patches on dev systems first unless actively exploited, coordinate maintenance windows with business), account remediation (reset all compromised credentials, revoke compromised API keys/tokens, rotate service account passwords, review privileged access). Rebuild vs Remediate Decision: Rebuild (ransomware full encryption, APT with extensive persistence, compliance requirement for forensic cleanliness, cost of investigation > rebuild time), Remediate (isolated malware infection, known good backups unavailable, business critical system can't afford downtime, containment verified effective). Verification: Malware-free confirmation (full EDR scan clean, memory analysis no suspicious processes, network traffic no C2 beaconing, 48-hour monitoring post-remediation), vulnerability validation (scan with Nessus/Qualys verify patch applied, penetration test exploit no longer works), access verification (all unauthorized accounts removed, privileged access reduced to least privilege, audit logs show no suspicious activity).

Recovery—Service Restoration Priority: Tier 1 Critical (authentication services domain controllers/Okta, email Exchange/Gmail, financial systems ERP/payment processing, customer-facing apps website/API), Tier 2 High (internal productivity tools Slack/Confluence/Jira, HR systems payroll/benefits, reporting/analytics dashboards), Tier 3 Standard (file shares, print services, non-critical applications). Backup Restoration: Validation before restore (daily automated restore tests to isolated environment, quarterly full DR exercise, verify backup not encrypted by ransomware), RPO/RTO adherence (critical systems: 1-hour RPO, 4-hour RTO, standard systems: 24-hour RPO, 48-hour RTO), integrity verification (hash comparison, antivirus scan before production, spot-check data accuracy with business users). Enhanced Monitoring: Additional detection rules (indicators of compromise IOCs from incident, hunt for similar TTPs, elevated logging on restored systems 30-day period), behavioral baselines (reestablish normal traffic patterns, user behavior analytics recalibration, anomaly detection threshold tuning), threat hunting (proactive search for similar threats, assume breach mentality, weekly hunts for 90 days post-incident). Validation Testing: Security scanning (Nessus vulnerability scan, penetration testing of attack vectors, configuration compliance CIS benchmarks), performance validation (application functionality testing, user acceptance testing, load testing for critical apps), compliance verification (audit log review, data integrity checks, regulatory requirements met).

Post-Incident—Lessons Learned Meeting: Within 2 weeks post-recovery, attendees (IR team, affected business units, IT operations, executive stakeholder, external IR firm if engaged), agenda (incident timeline reconstruction, what went well, what went poorly, root cause analysis 5 Whys, improvement recommendations), deliverables (formal incident report, action items with owners/due dates, budget requests for security enhancements). Root Cause Analysis: Technical root cause (unpatched vulnerability CVE-XXXX, phishing email bypassed email security, misconfigured firewall allowed lateral movement), process failures (lack of MFA enabled admin accounts, backups not tested, incident response plan outdated), people issues (user clicked phish despite training, IT admin used weak password, security alert ignored). Improvement Actions: Technical controls (deploy MFA on all accounts, implement EDR on all endpoints, enable network segmentation, upgrade SIEM to add use cases), process improvements (update IR playbooks, establish backup testing schedule, define escalation matrix, formalize change management), training enhancements (role-specific security training, phishing simulation frequency increase, tabletop exercise for exact incident type). Metrics Tracking: MTTD Mean Time to Detect (target <15 min, measure from compromise to alert generation), MTTA Mean Time to Acknowledge (target <5 min, measure from alert to analyst triage), MTTC Mean Time to Contain (target <1 hour P1, <4 hours P2), MTTR Mean Time to Recover (target <4 hours critical systems, <24 hours full recovery), incident trends (volume by type, severity distribution, false positive rate <10%). Continuous Improvement: Quarterly tabletop exercises (rotate scenarios: ransomware, data breach, DDoS, insider threat, supply chain compromise), annual red team exercise (full-scope attack simulation, test detection and response capabilities, purple team collaboration for improvement), playbook updates (after every incident, quarterly review cycle, annual overhaul), automation opportunities (SOAR playbook creation, ticket generation, containment actions).

Deliver incident response program as:

1. **IR PLAYBOOKS** - Ransomware, data breach, DDoS, phishing, insider threat, APT with step-by-step actions, decision trees, escalation paths
2. **DETECTION USE CASES** - 400+ SIEM rules mapped to MITRE ATT&CK, EDR behavioral analytics, threat intel integration, tuned <10% FP
3. **COMMUNICATION TEMPLATES** - Severity-based notifications for executives, customers, employees, regulators, media with approval workflows
4. **FORENSIC PROCEDURES** - Evidence collection (disk/memory/network), chain of custody, legal compliance, analysis tools/methods
5. **RECOVERY RUNBOOKS** - Service restoration priorities, backup procedures, validation testing, enhanced monitoring post-incident

---

## Usage Examples

### Example 1: SaaS Company Ransomware (Prevented)
**Prompt:** Incident response for SaaS company (500 employees, 10K customers, $50M revenue, AWS infrastructure, SOC 2 Type II) detecting ransomware before encryption.

**Output:** Detection—CrowdStrike EDR alert at 2:47 AM: suspicious PowerShell execution on finance workstation (user jsmith), process tree shows mshta.exe spawning PowerShell downloading payload from known ransomware C2 185.220.101.x (Recorded Future threat intel match), behavioral detection flags crypto API calls and rapid file modifications. MTTD: 3 minutes (automated alert), MTTA: 8 minutes (SOC analyst ack, escalate to tier 2). Investigation—Analyst reviews process execution (PowerShell decode base64 payload, downloads Conti ransomware variant, attempts to disable Windows Defender, enumerates network shares), EDR timeline shows user clicked phishing email attachment 2:44 AM (email security logs show it bypassed spam filter, sender spoofed finance@company.com), no lateral movement detected (ransomware still in initial access phase). Containment (2:55 AM, 8 min from detection)—EDR network isolation jsmith workstation (no network except CrowdStrike management), disable AD account jsmith + force logout all sessions, block C2 IP 185.220.101.x at perimeter firewall and Zscaler proxy, email quarantine all messages from spoofed sender domain-wide. Eradication—Memory dump captured pre-isolation (Volatility analysis confirms Conti variant, extract IOCs), wipe/reimage jsmith workstation from golden image (2 hours), reset jsmith credentials + require MFA re-enrollment, block file hash across all EDR agents (prevent reinfection). Recovery—Restore jsmith from 1-day-old backup to new workstation (30 min), validate no malware via full scan, user returns to work 3:45 AM (58 min total incident duration). Post-Incident—Root cause: phishing email bypassed Mimecast (new sender domain not in reputation DB), jsmith clicked despite security training (last training 6 months ago). Improvements: Deploy additional email security layer Abnormal Security (AI behavioral detection), increase phishing simulation frequency to monthly, implement application control AppLocker (block unsigned PowerShell scripts). Metrics: MTTD 3 min, MTTA 8 min, MTTC 8 min, MTTR 58 min. Cost: $15K (external IR consultation, AppLocker deployment, email security upgrade), $0 ransom paid, estimated $2M damage avoided (ransomware would have encrypted AWS RDS database, S3 buckets, disrupting service to 10K customers).

### Example 2: Healthcare Data Breach (100K Patient Records)
**Prompt:** Incident response for healthcare provider (200-bed hospital, 1,200 employees, Epic EHR, HIPAA compliance) discovering unauthorized access to 100K patient records via compromised credentials.

**Output:** Discovery—External notification from HHS OCR: patient filed complaint alleging unauthorized access to records (OCR audit finds Epic access logs show physician account drjones accessed 847 patient records not under care, including celebrities/VIPs). Timeline reconstruction via Protenus audit analytics: drjones account accessed 100K records over 18 months (2022-07 to 2024-01), access pattern shows 200-300 records/day during night shift (11 PM-3 AM when drjones not on call), records exported to CSV 127 times. Investigation—drjones interviewed by HR: admits selling patient data (names, DOB, SSN, diagnoses, medications) to identity theft ring for $50K (contacted via dark web forum), used credentials to access Epic from home laptop (bypassed MFA hospital network only requirement, VPN access didn't require MFA). Forensic analysis: drjones home laptop seized (FBI search warrant), forensic image shows CSV files sent via ProtonMail to buyer email, Bitcoin wallet transactions match dates of data exports. Scope Determination—100K unique patients affected (HIPAA breach threshold ≥500 requires HHS notification, state AG notification, media notification, individual notification), data elements: names, addresses, SSN, DOB, diagnoses, medications, insurance info (meets HIPAA definition of ePHI), no financial account numbers or credit cards (reduces risk of immediate fraud). Containment—Immediate: Disable drjones Epic access + AD account, reset credentials for all physicians (force password change + MFA re-enrollment), restrict VPN access (require MFA for all remote access, no exceptions). Legal/Regulatory—Outside counsel engaged (Nixon Peabody healthcare privacy team), FBI notified (identity theft ring, potential HIPAA criminal prosecution), HHS OCR breach report filed within 60 days, state AG notification (California, New York, 42 other states with breach laws), individual notification letters mailed within 60 days (first-class mail, Spanish translations for LA County patients). Notification Content: Description of breach (unauthorized access by employee), data elements involved (PHI but no financials), steps taken (terminated employee, enhanced monitoring, MFA), services offered (2 years free credit monitoring Experian, fraud alert assistance, identity theft insurance $1M policy). Media Notification—Press release (>500 CA residents triggers media notification requirement), spokesperson trained (CISO + PR firm), FAQ document for call center (200 agents handling patient inquiries). Remediation—Technical: Implement MFA for all Epic access (Imprivata for workstations, Duo for VPN, no SMS-based MFA), deploy Protenus audit analytics (weekly reports of anomalous access, break-the-glass monitoring, colleague/family access alerts), DLP for Epic exports (block CSV export, audit all report downloads, flag >100 record access per day). Administrative: Policy updates (sanction policy for snooping includes termination, annual privacy retraining emphasizes minimum necessary, background checks for all clinical staff annually). Post-Incident—Cost: $2.8M total (legal counsel $450K, forensic investigation $180K, notification letters/call center $380K, credit monitoring 100K patients × 2 years × $8/year = $1.6M, OCR settlement negotiation $200K). OCR Resolution Agreement—Corrective Action Plan: Implement enterprise-wide MFA (completed), deploy audit analytics (completed), update policies (completed), annual privacy training (ongoing), hire Privacy Officer (completed), third-party assessment (annually for 3 years). Financial Penalty: $850K (OCR settlement for willful neglect, mitigated by cooperation and prompt corrective action, avoided $50K per violation × 100K patients = $5B maximum penalty). Reputation Impact—Media coverage (local news, healthcare IT publications), patient trust surveys show 15% decline, 3-month decrease in elective procedures, 18-month recovery to baseline patient satisfaction. Lessons Learned—Root cause: Lack of MFA for remote access (allowed drjones home access), no audit analytics (18-month access went undetected), insufficient background checks (drjones had prior misdemeanor not flagged). Culture change: Monthly privacy newsletter highlights snooping consequences, terminate 2 additional employees for inappropriate access (send message), celebrate "privacy champion" staff who report concerns.

### Example 3: Financial Services DDoS Attack (Mitigated)
**Prompt:** Incident response for online bank (2,500 employees, 5M customers, $2B deposits, AWS/on-prem hybrid) experiencing 450 Gbps DDoS attack on customer-facing website.

**Output:** Detection—Cloudflare alerts at 9:47 AM: traffic spike from 5K req/sec baseline to 850K req/sec (170x increase), attack source: 50K IPs globally (botnet, likely Mirai variant), attack vector: HTTP flood GET requests to login page + search endpoints (application-layer DDoS). Impact—Website latency degrades from 200ms to 15 seconds (timeout threshold), login failures spike, customer service calls increase 300% (customers unable to access accounts), mobile app API responses degraded. MTTD: 2 minutes (Cloudflare automated alert, PagerDuty notification to on-call engineer). Analysis—Attack characteristics: Randomized User-Agent headers (evade simple rate limiting), distributed source IPs (no single ISP to coordinate block), legitimate-seeming HTTP requests (not SYN flood, requires application-layer mitigation), sustained attack (not burst, indicates organized threat actor). Attacker Attribution: Dark web monitoring finds claim of responsibility by hacktivist group "OpFinance" (anti-bank ideology, demands bank donate $10M to poverty relief or attack continues), threat intel indicates group previously attacked 3 European banks (sustained DDoS 48-72 hours). Containment (9:49 AM, 2 min from detection)—Cloudflare aggressive DDoS mitigation mode (JavaScript challenge for all visitors, CAPTCHA for suspicious IPs, rate limiting 100 req/min per IP), CDN caching aggressive (cache login page HTML, API responses for 30 seconds, reduce origin server load), AWS Auto Scaling triggered (scale web tier from 20 to 200 EC2 instances, ALB distributes load, RDS read replicas scaled 5 to 20). Results: 9:55 AM (8 min from detection) website latency returns to 400ms (2x baseline but functional), login success rate 85% (vs 20% during attack peak), customer complaints stabilize. Eradication—Cloudflare blocks 98% of attack traffic (450 Gbps inbound, 9 Gbps reaches origin), mitigation rules tuned (whitelist known-good customer IPs, aggressive blocking of data center IPs, geo-blocking from attack source countries non-customer regions), threat intel shared with FS-ISAC (other banks apply preventative blocks). Attack duration: 26 hours (sustained until OpFinance claims victory via Twitter, likely satisfied with media attention). Recovery—Cloudflare mitigations gradually relaxed (11 AM day 2, return to normal security posture), Auto Scaling scaled down (cost optimization, maintain 50 instances vs 20 baseline for buffer), post-incident monitoring (72-hour elevated logging, watch for follow-on attacks). Communication—Customers: Status page updated every 2 hours (We are experiencing high traffic volumes, service may be degraded, we are working to resolve), no breach notification required (no data accessed, availability impact only). Media: Press release (Our systems successfully defended against a cyber attack, customer data remains secure, no downtime occurred). Regulators: OCC notification (significant operational disruption, voluntary notification), FFIEC report (cyber incident report within 72 hours, detail mitigation actions). Post-Incident—Cost: $37K total (Cloudflare overage charges $18K for 450 Gbps, AWS auto-scaling $12K for 200 instances × 26 hours, incident response labor $7K for weekend on-call). Lessons Learned—Root cause: DDoS mitigation reliant on single provider (Cloudflare), no volumetric DDoS testing (largest prior test 10 Gbps), incident playbook assumed smaller attack. Improvements: Multi-CDN strategy (add Akamai as secondary, failover if Cloudflare overwhelmed), increase DDoS testing (annual 100 Gbps test with AttackIQ, quarterly tabletop exercises), ransom policy formalization (executive decision matrix for extortion demands, legal counsel pre-engaged). Metrics: MTTD 2 min, MTTC 8 min, MTTR 26 hours (attack sustained), availability 98.7% during attack (vs 99.95% SLA baseline), zero data breach (attack vector didn't involve unauthorized access). Reputation: Positive media coverage (Bank successfully defends against massive cyber attack, industry publication cites as DDoS response best practice), customer surveys show 92% confidence in bank's security (post-incident vs 89% pre-incident).

---

## Cross-References

- [Incident Response Plan](incident-response.md) - Detailed NIST 800-61 implementation guide
- [SIEM Security Monitoring](../Security-Operations/siem-security-monitoring.md) - Detection engineering and log analysis
